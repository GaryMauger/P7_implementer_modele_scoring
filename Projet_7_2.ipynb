{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0851f35-dabe-4dc0-aa78-f341a2f72c6c",
   "metadata": {},
   "source": [
    "<center><span style=\"font-size:28px; font-weight:bold;\">Implementez un modèle de scoring</span><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea904f07-dbdd-42ef-9a91-2a05db218a77",
   "metadata": {},
   "source": [
    "<center><span style=\"font-size:28px; font-weight:bold;\">2 - Modélisation</span><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb20fc99-7e0c-4447-b776-f44fce2bbc91",
   "metadata": {},
   "source": [
    "# Sommaire\n",
    "\n",
    "1. [Importation des librairies et des données](#1)\n",
    "2. [Première modélisation : avec déséquilibre de classe](#2)\n",
    "   - [Dummy Classifier](#2.1)\n",
    "   - [Régression logistique](#2.2)\n",
    "   - [Gradient Boosting](#2.3)\n",
    "3. [Deuxième modélisation : après SMOTE](#3)\n",
    "   - [Application du SMOTE](#3.1)\n",
    "   - [Entraînement des modèles](#3.2)\n",
    "   - [Évaluation du modèle](#3.3)\n",
    "4. [Recherche des meilleurs hyperparamètres](#4)\n",
    "   - [LGBM Classifier](#4.1)\n",
    "   - [XGBoost Classifier](#4.2)\n",
    "5. [Ajout d'un score métier](#5)\n",
    "   - [Création du score métier](#5.1)\n",
    "   - [Evaluation du score métier](#5.2)\n",
    "   - [Essais empiriques pour ajuster cost_fn et cost_fp](#5.3)\n",
    "6. [Modèle final optimisé](#6)\n",
    "   - [Analyse des différents seuils et coûts](#6.1)\n",
    "   - [Meilleures configurations en fonction du contexte](#6.2)\n",
    "   - [Analyse de ces différentes configurations](#6.3)\n",
    "7. [Features importance](#7)\n",
    "8. [Conclusion](#8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad8a951-04de-4306-9b24-8884b41496a8",
   "metadata": {},
   "source": [
    "- Dans la première modélisation on va faire une sous partie par modèle pour bien expliquer tout. \n",
    "- Dans la deuxièeme on va fire une partie sur SMOTE pour expliquer le processus, puis une sous partie avec la modélisation, on peut faire un seul bloc de code pour la modélisation des 3 modèles. On peut également réserver une sous partie à l'évaluation. Attentin, faire un graphique pour analyser l'AUC après l'évaluation sur train (avec cross validation). (On peut utiliser le test set uniquement pour valider la modélisation finale ?)\n",
    "- Idem que précédemment pour la partie après ajout du score métier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9584f069-2944-4ac4-be20-7a66940e7757",
   "metadata": {},
   "source": [
    "# 1 - Importation des librairies et des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d215f8-ffb6-4838-8289-cc907c933e30",
   "metadata": {},
   "source": [
    "## 1.1 - Importation des différentes librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef9b683f-0555-4bd4-a0b5-a169d11b977e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# numpy and pandas for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "# File system manangement\n",
    "import os\n",
    "\n",
    "# Suppress warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# matplotlib and seaborn for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Autres bibliothèques utiles :\n",
    "import time\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import mlflow\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, accuracy_score, precision_recall_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from mlflow.models import infer_signature\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "import shap\n",
    "\n",
    "import pipeline_features_eng\n",
    "\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "from evidently.report import Report\n",
    "from evidently.metrics import DataDriftTable\n",
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35609fc4-9dbf-4ee1-bbc5-c8c7c5e9f06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On va démarrer le chronomètre\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7348d7-68f2-4cda-a110-a4d21873aab9",
   "metadata": {},
   "source": [
    "## 1.2 - Configuration de Mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb4824d-50f0-4806-b109-ab79a80e66d3",
   "metadata": {},
   "source": [
    "**MLflow** est une plateforme open source conçue pour gérer le cycle de vie des modèles de machine learning. Il permet de suivre les expérimentations, de versionner les modèles et de faciliter leur déploiement. \n",
    "\n",
    "Dans notre projet, **MLflow** est utilisé pour :\n",
    "\n",
    "- **Suivre les runs** et les résultats des expérimentations.\n",
    "- **Enregistrer les paramètres, métriques et artefacts** liés aux différents modèles.\n",
    "- **Versionner les modèles optimisés** pour un futur déploiement.\n",
    "\n",
    "**A - Les runs dans MLflow**\n",
    "\n",
    "Un **run** dans MLflow correspond à une exécution de code qui est suivie, enregistrant des informations pertinentes telles que :\n",
    "\n",
    "- Les **paramètres** d'entraînement utilisés (par exemple, les hyperparamètres comme le taux d'apprentissage ou le nombre d'estimations).\n",
    "- Les **métriques** calculées (AUC, précision, temps d'exécution, etc.).\n",
    "- Les **artefacts** produits (graphiques, modèles enregistrés, etc.).\n",
    "- La **signature des données** (la structure des entrées et sorties du modèle).\n",
    "\n",
    "**B - Enregistrement des modèles dans MLflow**\n",
    "\n",
    "MLflow permet d’enregistrer les modèles pour une future utilisation via l'API **`mlflow.log_model()`**. Les modèles peuvent être :\n",
    "\n",
    "- **Versionnés** et **stockés** avec les artefacts.\n",
    "- **Récupérés** et **chargés** plus tard pour faire des prédictions ou les déployer dans des pipelines de production.\n",
    "\n",
    "**C - Suivi des résultats avec MLflow**\n",
    "\n",
    "Grâce à MLflow, nous pouvons consulter toutes nos expérimentations, comparer les runs, et identifier les meilleurs modèles en fonction des métriques enregistrées. MLflow permet également de visualiser des graphiques et de suivre les versions des modèles pour garder un historique détaillé des itérations de notre travail.\n",
    "\n",
    "Par exemple, dans le **UI de MLflow**, nous pouvons facilement :\n",
    "\n",
    "- **Comparer plusieurs runs** côte à côte pour voir quelle configuration a donné les meilleurs résultats.\n",
    "- **Visualiser les métriques** pour chaque run et ajuster nos modèles en conséquence.\n",
    "\n",
    "**D - Versioning des modèles**\n",
    "\n",
    "MLflow offre également la possibilité de **versionner** les modèles grâce à un modèle registry. Chaque modèle enregistré dans le **Model Registry** peut être associé à une version, ce qui facilite le déploiement et la gestion continue des versions en production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cd18832-e72c-4e0b-aeac-db06f35abd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On va configurer l'URI du serveur MLflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")  # Remplacer par l'URI de notre serveur si nécessaire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4ed021-8f4b-4cfc-b4c8-0849d02e23c1",
   "metadata": {},
   "source": [
    "Le code `mlflow.set_tracking_uri(\"http://localhost:5000\")` sert à configurer l'URI du serveur de tracking MLflow.\n",
    "\n",
    "   - L'URI spécifie où se trouve le serveur MLflow. Par défaut, MLflow utilise un serveur local en mémoire, ce qui signifie que les données ne seront pas conservées après l'arrêt de la session.\n",
    "   - En définissant l'URI, nous pouvons nous connecter à un serveur MLflow qui stocke les résultats des expérimentations de manière persistante, ce qui permet d'accéder aux résultats ultérieurement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcd2652-a2a3-46d2-a3aa-46e6c1163bd9",
   "metadata": {},
   "source": [
    "## 1.3 - Importation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f55d07d-426b-4ad9-aab8-cb0a3ed8224c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spécifions les différents chemins possibles\n",
    "directories = [\n",
    "    \"C:\\\\Users\\\\mauge\\\\Documents\\\\github\\\\P7_implementer_modele_scoring\",\n",
    "    \"C:/Users/mauge/Documents/github/P7_implementer_modele_scoring/Projet+Mise+en+prod+-+home-credit-default-risk/\",\n",
    "    \"/Users/Gary/Documents/GitHub/FormationData/DataScientist_P7/Projet+Mise+en+prod+-+home-credit-default-risk\",\n",
    "    \"/Users/Gary/Documents/GitHub/P7_implementer_modele_scoring\",\n",
    "    \"C:\\\\Users\\\\mauge\\\\OneDrive\\\\Bureau\\\\Projet+Mise+en+prod+-+home-credit-default-risk\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43e07c9b-3e41-49d0-a97d-906e79f7f0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(file_name, directory_options, **kwargs):\n",
    "    for directory in directory_options:\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"Loading {file_name} from {file_path}\")\n",
    "            return pd.read_csv(file_path, **kwargs)\n",
    "    print(f\"{file_name} not found in any of the specified directories.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c9f17e6-80a1-4e6a-9d75-a4514bb6fb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading df_data_6.csv from C:\\Users\\mauge\\Documents\\github\\P7_implementer_modele_scoring\\df_data_6.csv\n",
      "Loading application_train.csv from C:/Users/mauge/Documents/github/P7_implementer_modele_scoring/Projet+Mise+en+prod+-+home-credit-default-risk/application_train.csv\n",
      "Loading application_test.csv from C:/Users/mauge/Documents/github/P7_implementer_modele_scoring/Projet+Mise+en+prod+-+home-credit-default-risk/application_test.csv\n"
     ]
    }
   ],
   "source": [
    "df_data_6 = load_csv('df_data_6.csv', directories)\n",
    "df_application_train = load_csv('application_train.csv', directories)\n",
    "df_application_test = load_csv('application_test.csv', directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "746aded5-356c-4109-9337-45c7dac9e1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 356255 entries, 0 to 356254\n",
      "Columns: 333 entries, SK_ID_CURR to EMERGENCYSTATE_MODE_Yes\n",
      "dtypes: float64(333)\n",
      "memory usage: 905.1 MB\n"
     ]
    }
   ],
   "source": [
    "df_data_6.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71a52063-f088-47f2-941d-f31844d50bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparation de df_data_4 en train et test\n",
    "df_train = df_data_6[df_data_6['SK_ID_CURR'].isin(df_application_train['SK_ID_CURR'])]\n",
    "df_test = df_data_6[df_data_6['SK_ID_CURR'].isin(df_application_test['SK_ID_CURR'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05665e3d-f1a4-423e-a605-ffe566ebcc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardization complete.\n",
      "Data shape after scaling:  (307511, 333)\n"
     ]
    }
   ],
   "source": [
    "# Identifiants et cible à exclure de la standardisation\n",
    "exclude_cols = ['SK_ID_CURR', 'TARGET']\n",
    "\n",
    "# Séparer les colonnes à standardiser de celles à exclure\n",
    "cols_to_scale = [col for col in df_train.columns if col not in exclude_cols]\n",
    "df_train_to_scale = df_train[cols_to_scale]\n",
    "df_train_excluded = df_train[exclude_cols]\n",
    "\n",
    "# Création du scaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Ajustement et transformation\n",
    "df_train_scaled = scaler.fit_transform(df_train_to_scale)\n",
    "\n",
    "# Recréer un DataFrame avec les colonnes standardisées\n",
    "df_train_scaled_df = pd.DataFrame(df_train_scaled, columns=cols_to_scale)\n",
    "\n",
    "# Ajouter les colonnes exclues\n",
    "train = pd.concat([df_train_scaled_df, df_train_excluded.reset_index(drop=True)], axis=1)\n",
    "\n",
    "print('Standardization complete.')\n",
    "print('Data shape after scaling: ', train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc2126b4-1675-4f55-9709-45980999e105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparation des features (X) et de la target (y)\n",
    "X_train = train.drop(['TARGET', 'SK_ID_CURR'], axis=1)  # Retirer SK_ID_CURR\n",
    "y_train = train['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "650dad41-a13b-4147-8de8-8d224ef62869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features shape with categorical columns:  (307511, 122)\n",
      "Testing Features shape with categorical columns:  (48744, 121)\n"
     ]
    }
   ],
   "source": [
    "print('Training Features shape with categorical columns: ', df_application_train.shape)\n",
    "print('Testing Features shape with categorical columns: ', df_application_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90be531f-0ff3-4817-ba4b-0c06b5164602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique SK_ID_CURR in df_application_test: 48744\n",
      "Unique SK_ID_CURR in df_data_4: 356255\n"
     ]
    }
   ],
   "source": [
    "# Vérifier les valeurs uniques de SK_ID_CURR dans df_application_test et df_data_4\n",
    "print(\"Unique SK_ID_CURR in df_application_test:\", df_application_test['SK_ID_CURR'].nunique())\n",
    "print(\"Unique SK_ID_CURR in df_data_4:\", df_data_6['SK_ID_CURR'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bde478ce-3085-4ecb-9a50-3a19eac42f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On va séparer l'ensemble d'entraînement en un ensemble d'entraînement et un ensemble de validation\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5cb474d-28bb-4a84-8eda-0ab6336f3046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille de X_train_split : (215257, 331)\n",
      "Taille de y_train_split : (215257,)\n",
      "Taille de X_val : (92254, 331)\n",
      "Taille de y_val : (92254,)\n"
     ]
    }
   ],
   "source": [
    "# Affichage des tailles des datasets\n",
    "print(f\"Taille de X_train_split : {X_train_split.shape}\")\n",
    "print(f\"Taille de y_train_split : {y_train_split.shape}\")\n",
    "print(f\"Taille de X_val : {X_val.shape}\")\n",
    "print(f\"Taille de y_val : {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcef6912-69f0-44fd-9477-d7ef810798f1",
   "metadata": {},
   "source": [
    "## 1.4 - Définition des fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f1ef9f-7796-4687-91c4-56271d21e31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models_with_cross_validation(models, X_train_split, y_train_split, log_to_mlflow=True):\n",
    "    # Initialisation d'un DataFrame pour stocker les résultats\n",
    "    results_df = pd.DataFrame(columns=['Model', 'AUC_mean', 'Accuracy_mean', 'Fit_time_mean', 'Score_time_mean'])\n",
    "\n",
    "    for name, model in models.items():\n",
    "        if log_to_mlflow:\n",
    "            # Nom de la run MLflow avec le nom du modèle\n",
    "            mlflow_run_name = f\"{name}\"\n",
    "\n",
    "            # Démarre une nouvelle run MLflow\n",
    "            with mlflow.start_run(run_name=mlflow_run_name):\n",
    "                # Mesurer le temps de fitting\n",
    "                start_fit_time = time.time()\n",
    "                \n",
    "                # Évaluation via cross-validation avec prédictions\n",
    "                y_pred = cross_val_predict(model, X_train_split, y_train_split, cv=5, method='predict_proba', n_jobs=-1)\n",
    "                \n",
    "                end_fit_time = time.time()\n",
    "                fit_time_mean = end_fit_time - start_fit_time\n",
    "\n",
    "                # Calcul des scores\n",
    "                auc_mean = roc_auc_score(y_train_split, y_pred[:, 1])\n",
    "                accuracy_mean = accuracy_score(y_train_split, (y_pred[:, 1] >= 0.5).astype(int))\n",
    "\n",
    "                # Mesurer le temps de scoring\n",
    "                start_score_time = time.time()\n",
    "                model.fit(X_train_split, y_train_split)  # Fit le modèle pour le scoring\n",
    "                end_score_time = time.time()\n",
    "                score_time_mean = end_score_time - start_score_time\n",
    "\n",
    "                # Logging dans MLFlow\n",
    "                mlflow.log_param(\"Model\", name)\n",
    "                mlflow.log_metric(\"AUC\", auc_mean)\n",
    "                mlflow.log_metric(\"Accuracy\", accuracy_mean)\n",
    "                mlflow.log_metric(\"Fit_time_mean\", fit_time_mean)\n",
    "                mlflow.log_metric(\"Score_time_mean\", score_time_mean)\n",
    "\n",
    "                # Générer la signature du modèle (entrées/sorties)\n",
    "                signature = infer_signature(X_train_split, (y_pred[:, 1] >= 0.5).astype(int))\n",
    "\n",
    "                # Enregistrement du modèle dans MLFlow et Model Registry avec versioning\n",
    "                mlflow.sklearn.log_model(\n",
    "                    model,\n",
    "                    artifact_path=\"model\",\n",
    "                    signature=signature,\n",
    "                    registered_model_name=f\"{name}-classification-model\"  # Enregistrement avec nom et versioning\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            # Évaluation via cross-validation sans logging\n",
    "            y_pred = cross_val_predict(model, X_train_split, y_train_split, cv=5, method='predict_proba', n_jobs=-1)\n",
    "            auc_mean = roc_auc_score(y_train_split, y_pred[:, 1])\n",
    "            accuracy_mean = accuracy_score(y_train_split, (y_pred[:, 1] >= 0.5).astype(int))\n",
    "\n",
    "        # Stockage dans DataFrame des résultats\n",
    "        new_row = pd.DataFrame({\n",
    "            'Model': [name],\n",
    "            'AUC_mean': [auc_mean],\n",
    "            'Accuracy_mean': [accuracy_mean],\n",
    "            'Fit_time_mean': [fit_time_mean if log_to_mlflow else None],\n",
    "            'Score_time_mean': [score_time_mean if log_to_mlflow else None],\n",
    "        })\n",
    "        \n",
    "        results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9aa774-950a-407b-b7da-fdee9bce9091",
   "metadata": {},
   "source": [
    "La fonction `evaluate_models_with_cross_validation` évalue les modèles via validation croisée et logue les résultats dans MLflow si demandé.\n",
    "    \n",
    "Parameters:\n",
    "    - models (dict): Dictionnaire contenant les noms et instances des modèles à évaluer.\n",
    "    - X_train_split (pd.DataFrame): Données d'entraînement.\n",
    "    - y_train_split (pd.Series): Cibles d'entraînement.\n",
    "    - log_to_mlflow (bool): Si True, enregistre les résultats dans MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a9ba2c-28d3-4a54-a489-74340a3d3446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train_split, y_train_split, X_val, y_val, model_name, retrain=False, log_to_mlflow=True):\n",
    "    # Si l'enregistrement dans MLflow est activé\n",
    "    if log_to_mlflow:\n",
    "        # Nom de la run MLflow avec le suffixe \"TEST\"\n",
    "        mlflow_run_name = f\"{model_name} test_set\"\n",
    "\n",
    "        # Démarre une nouvelle run MLflow\n",
    "        with mlflow.start_run(run_name=mlflow_run_name):\n",
    "            if retrain:\n",
    "                model.fit(X_train_split, y_train_split)\n",
    "\n",
    "            # Fait les prédictions\n",
    "            start_time = time.time()\n",
    "            y_pred_proba = model.predict_proba(X_val)[:, 1]  # Probabilités pour la classe positive\n",
    "            y_pred = model.predict(X_val)\n",
    "            prediction_time = time.time() - start_time\n",
    "\n",
    "            # Calcule les métriques\n",
    "            accuracy = accuracy_score(y_val, y_pred)\n",
    "            roc_auc = roc_auc_score(y_val, y_pred_proba)\n",
    "            precision = precision_score(y_val, y_pred)\n",
    "            recall = recall_score(y_val, y_pred)\n",
    "            f1 = f1_score(y_val, y_pred)\n",
    "            cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "            # Logue les métriques dans MLflow\n",
    "            mlflow.log_metric(\"Accuracy\", accuracy)\n",
    "            mlflow.log_metric(\"AUC\", roc_auc)\n",
    "            mlflow.log_metric(\"Precision\", precision)\n",
    "            mlflow.log_metric(\"Recall\", recall)\n",
    "            mlflow.log_metric(\"F1-score\", f1)\n",
    "            mlflow.log_metric(\"Score_time\", prediction_time)\n",
    "\n",
    "            # Enregistre le modèle dans MLflow\n",
    "            mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "            # Affiche les graphiques\n",
    "            plot_confusion_matrix_and_roc(cm, y_val, y_pred_proba, roc_auc)\n",
    "\n",
    "            # Enregistre les graphiques dans MLflow\n",
    "            fig = plt.gcf()  # Récupérer la figure actuelle\n",
    "            mlflow.log_figure(fig, \"confusion_matrix_and_roc_curve.png\")\n",
    "\n",
    "    else:\n",
    "        # Si l'enregistrement dans MLflow est désactivé\n",
    "        if retrain:\n",
    "            model.fit(X_train_split, y_train_split)\n",
    "\n",
    "        # Fait les prédictions\n",
    "        start_time = time.time()\n",
    "        y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "        y_pred = model.predict(X_val)\n",
    "        prediction_time = time.time() - start_time\n",
    "\n",
    "        # Calcule les métriques\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        roc_auc = roc_auc_score(y_val, y_pred_proba)\n",
    "        precision = precision_score(y_val, y_pred)\n",
    "        recall = recall_score(y_val, y_pred)\n",
    "        f1 = f1_score(y_val, y_pred)\n",
    "        cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "        # Affiche les résultats sans MLflow\n",
    "        print(f'Accuracy: {accuracy:.4f}')\n",
    "        print(f'AUC: {roc_auc:.4f}')\n",
    "        print(f'Precision: {precision:.4f}')\n",
    "        print(f'Recall: {recall:.4f}')\n",
    "        print(f'F1-score: {f1:.4f}')\n",
    "        print(\"\\nMatrice de confusion :\")\n",
    "        print(pd.DataFrame(cm, index=['True Negative', 'True Positive'], columns=['Predicted Negative', 'Predicted Positive']))\n",
    "\n",
    "        # Affiche les graphiques\n",
    "        plot_confusion_matrix_and_roc(cm, y_val, y_pred_proba, roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7a8be4-9bad-4853-80b7-da00d2851c8e",
   "metadata": {},
   "source": [
    "La fonction `evaluate_model` permet d'évaluer les performances d'un modèle de classification sur un ensemble de données de validation. Voici les principales métrics enregistrées par cette fonction :\n",
    "\n",
    "L'**accuracy** : La précision du modèle est calculée à l'aide de l'accuracy score.\n",
    "\n",
    "La **précision** mesure la proportion de prédictions positives qui sont effectivement correctes. C'est une métrique particulièrement utile dans les situations où le coût des faux positifs est élevé. Par exemple, dans un modèle de scoring de crédit, une faible précision signifierait que le modèle accorde trop souvent des crédits à des clients qui ne rembourseront pas.\n",
    "\n",
    "Le **rappel** (ou sensibilité) mesure la proportion de véritables positifs correctement identifiés parmi l'ensemble des cas positifs réels. Le rappel est essentiel dans les scénarios où les faux négatifs (manquer des clients à risque) sont coûteux. Dans un modèle de scoring, un faible rappel signifie que le modèle ne détecte pas suffisamment de clients à risque.\n",
    "\n",
    "Le **F1-score** est la moyenne harmonique de la précision et du rappel. Il est utile pour trouver un équilibre entre les deux, particulièrement quand il existe un déséquilibre entre les classes positives et négatives. Le F1-score est une bonne mesure globale dans des situations où il faut à la fois maximiser la précision et le rappel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def9fb4f-1afb-4733-a324-afb4c293890c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix_and_roc(cm, y_val, y_pred_proba, roc_auc):\n",
    "    # Fonction pour tracer la matrice de confusion et la courbe ROC\n",
    "    fpr, tpr, _ = roc_curve(y_val, y_pred_proba)\n",
    "\n",
    "    # Affiche les deux graphiques côte à côte\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "    # Matrice de confusion\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax[0], annot_kws={\"size\": 16})\n",
    "    ax[0].set_xlabel('Predicted', fontsize=14)\n",
    "    ax[0].set_ylabel('True', fontsize=14)\n",
    "    ax[0].set_title('Matrice de confusion', fontsize=16)\n",
    "\n",
    "    # Courbe ROC\n",
    "    ax[1].plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    ax[1].plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "    ax[1].set_xlim([0.0, 1.0])\n",
    "    ax[1].set_ylim([0.0, 1.05])\n",
    "    ax[1].set_xlabel('False Positive Rate', fontsize=14)\n",
    "    ax[1].set_ylabel('True Positive Rate', fontsize=14)\n",
    "    ax[1].set_title('Courbe ROC', fontsize=16)\n",
    "    ax[1].legend(loc='lower right', fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b69ae2-d472-431f-b7dd-1c559bb934fb",
   "metadata": {},
   "source": [
    "La fonction `plot_confusion_matrix_and_roc` affiche une matrice de confusion ainsi que la courbe AUC-ROC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253aa9b1-6419-46ca-8bd6-86b6153fc838",
   "metadata": {},
   "source": [
    "La **matrice de confusion** est un tableau qui permet de visualiser les performances d'un modèle de classification. Elle présente les comparaisons entre les vraies valeurs et les valeurs prédites par le modèle. Les quatre composantes de la matrice sont les suivantes :\n",
    "- **Vrais Positifs (TP)** : Nombre de prédictions positives correctes (le modèle prédit que l'événement se produit et c'est effectivement le cas).\n",
    "- **Faux Positifs (FP)** : Nombre de prédictions positives incorrectes (le modèle prédit que l'événement se produit, mais ce n'est pas le cas).\n",
    "- **Vrais Négatifs (TN)** : Nombre de prédictions négatives correctes (le modèle prédit que l'événement ne se produit pas et c'est effectivement le cas).\n",
    "- **Faux Négatifs (FN)** : Nombre de prédictions négatives incorrectes (le modèle prédit que l'événement ne se produit pas, mais il se produit en réalité).\n",
    "\n",
    "Cette matrice est utile pour calculer des métriques telles que la **précision** (accuracy), la **précision** (precision), le **rappel** (recall), et le **F1-score**, qui sont des indicateurs clés de la performance globale du modèle.\n",
    "\n",
    "La **courbe ROC (Receiver Operating Characteristic)** est un graphique qui évalue la capacité d'un modèle à distinguer entre les classes positives et négatives à différents seuils de décision. Chaque point de la courbe correspond à un certain seuil de classification. La courbe est tracée en fonction des axes suivants :\n",
    "- **Axe des X (False Positive Rate, FPR)** : Taux de faux positifs, calculé comme le ratio entre les faux positifs et l'ensemble des réels négatifs (FP / (FP + TN)).\n",
    "- **Axe des Y (True Positive Rate, TPR)** : Taux de vrais positifs (ou sensibilité), calculé comme le ratio entre les vrais positifs et l'ensemble des réels positifs (TP / (TP + FN)).\n",
    "\n",
    "Un modèle parfait aurait une courbe ROC qui passe par le coin supérieur gauche du graphique, ce qui signifierait un taux de vrais positifs de 1 et un taux de faux positifs de 0.\n",
    "\n",
    "L'**aire sous la courbe ROC (AUC)** est une mesure globale de la performance du modèle. Elle indique dans quelle mesure le modèle est capable de distinguer correctement les classes. L'aire sous la courbe varie entre 0 et 1 :\n",
    "- **AUC = 1** : Le modèle est parfait.\n",
    "- **AUC = 0.5** : Le modèle n'est pas meilleur qu'un classificateur aléatoire.\n",
    "- **AUC < 0.5** : Le modèle se comporte pire qu'un classificateur aléatoire, indiquant une mauvaise classification des classes.\n",
    "\n",
    "Une **AUC élevée** indique que le modèle fait bien la distinction entre les classes positives et négatives, même à différents seuils de classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54404320-e5b3-495c-b277-900277691b9c",
   "metadata": {},
   "source": [
    "# 2 - Première modélisation : avec déséquilibre de classe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d318c0-bb22-4d4e-a18b-676d80c95f8e",
   "metadata": {},
   "source": [
    "Dans notre cas, l'ensemble de test (`TEST`) ne contient pas la variable cible (`TARGET`). Par conséquent, nous ne pouvons pas évaluer directement les performances de nos modèles sur cet ensemble. Pour contourner ce problème et garantir une évaluation fiable, nous séparons l'ensemble d'entraînement (`TRAIN`) en deux sous-ensembles : un sous-ensemble d'entraînement et un sous-ensemble de validation.\n",
    "\n",
    "Cette séparation nous permet de :\n",
    "1. **Entraîner les Modèles** : Utiliser le sous-ensemble d'entraînement pour ajuster les paramètres du modèle.\n",
    "2. **Évaluer les Modèles** : Utiliser le sous-ensemble de validation, qui contient les labels de la variable cible, pour évaluer les performances du modèle avant de le tester sur l'ensemble de test. \n",
    "\n",
    "Cela assure que le modèle est bien évalué et validé même si l'ensemble de test ne fournit pas de `TARGET`, nous permettant ainsi de juger de la performance et de la robustesse du modèle de manière fiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53c614c-83da-4bd5-847b-eed16c210aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On va commencer par initialiser notre expérimentation MLFlow\n",
    "mlflow.set_experiment(\"1 - Première modélisation : avec déséquilibre de classe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45136168-3fe6-444e-bf17-ab449a157e2b",
   "metadata": {},
   "source": [
    "Dans **MLflow**, il est recommandé de faire la distinction entre **expérimentations** et **runs** :\n",
    "\n",
    "1. **Expérimentations** :\n",
    "   - Une **expérimentation** est un groupe de runs associés à un même objectif.\n",
    "   - En initialisant l'expérimentation avec `mlflow.set_experiment(\"Home Credit Default Risk - Modelisation 1\")`, nous créons un contexte pour toutes les runs que nous allons effectuer pour cet objectif.\n",
    "\n",
    "2. **Runs** :\n",
    "   - Un **run** représente une exécution d'un modèle. Chaque fois que nus allons entraîner un modèle, explorer une nouvelle méthode, ou modifier des hyperparamètres, nous allons créer une nouvelle run.\n",
    "   - Cela va nous permettre de suivre les métriques, les paramètres, et les artefacts (comme les modèles entraînés) pour chaque exécution de manière isolée."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b08bde1-bf65-4ead-9008-b822eaf32465",
   "metadata": {},
   "source": [
    "## 2.1 - Dummy Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d127e84-1f6a-4585-9a88-0f28ac18ed09",
   "metadata": {},
   "source": [
    "Le **Dummy Classifier** est un modèle de référence (ou \"baseline\") utilisé dans le domaine du machine learning pour fournir un point de comparaison simple lorsque l'on évalue les performances d'un modèle plus complexe. Contrairement aux modèles traditionnels qui tentent de trouver des patterns significatifs dans les données pour effectuer des prédictions, le Dummy Classifier adopte des stratégies très simples pour effectuer ses prédictions, sans essayer de modéliser la relation entre les caractéristiques et la cible.\n",
    "\n",
    "L'objectif principal du Dummy Classifier est de servir de référence minimale. En l'utilisant, on peut mesurer à quel point un modèle plus sophistiqué surpasse une méthode naïve. Il est particulièrement utile dans les cas où les données sont déséquilibrées, car il permet de voir à quel point un modèle avancé est capable de capturer des patterns qui surpassent une stratégie simple.\n",
    "\n",
    "Le Dummy Classifier peut utiliser différentes stratégies pour faire ses prédictions :\n",
    "1. **\"stratified\"** : Les prédictions sont générées en respectant les proportions des différentes classes dans les données d'entraînement.\n",
    "2. **\"most_frequent\"** : Le classificateur prédit toujours la classe la plus fréquente dans les données d'entraînement. C'est utile pour voir comment un modèle sophistiqué surperforme par rapport à cette simple stratégie.\n",
    "3. **\"prior\"** : Cette stratégie prédit les classes en fonction de la probabilité a priori des classes d'entraînement.\n",
    "4. **\"uniform\"** : Le classificateur effectue des prédictions au hasard, en tirant les classes de manière uniforme.\n",
    "5. **\"constant\"** : Le classificateur prédit toujours une classe constante spécifiée par l'utilisateur.\n",
    "\n",
    "Dans notre cas de données déséquilibrées, la stratégie **Most Frequent** serait souvent utilisée pour représenter la baseline la plus simple et pertinente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5bb7a5-306b-4a48-8a95-496a42ab0dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On va définir le/les modèle(s)\n",
    "models = {\n",
    "    'Dummy Classifier': DummyClassifier(strategy='most_frequent')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4256d2e-8647-429d-b8f6-c669e1bfe31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On va entraîner et évaluer le modèle via cross validation sur le train_set\n",
    "evaluate_models_with_cross_validation(models, X_train_split, y_train_split, log_to_mlflow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4c85b1-ae4f-433c-9d50-6a48ab41d3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On va évaluer le modèle sur le test_set et analyser les graphiques\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Évaluation du modèle: {model_name}\")\n",
    "    evaluate_model(model, X_train_split, y_train_split, X_val, y_val, model_name, retrain=True, log_to_mlflow=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff5f947-1e78-471f-b3a0-5d916b52fb26",
   "metadata": {},
   "source": [
    "- **AUC_mean** : 0.5  \n",
    "  Cela indique que le modèle a une performance au niveau du hasard pour la classification binaire. Une AUC de 0.5 signifie que le modèle ne distingue pas mieux entre les classes que si les prédictions étaient aléatoires.\n",
    "\n",
    "- **Accuracy_mean**\n",
    "  L'accuracy semble élevée, mais pour un `Dummy Classifier` utilisant la stratégie `most_frequent`, ce score élevé pourrait indiquer que la classe majoritaire occupe une proportion importante dans les données. Cela signifie que le modèle prédit la classe la plus fréquente, ce qui peut donner une apparence trompeusement bonne de performance dans des cas de déséquilibre des classes.\n",
    "\n",
    "- **Fit_time_mean**   \n",
    "  Le temps moyen nécessaire pour entraîner le modèle est assez faible, ce qui est attendu car le `Dummy Classifier` est très simple.\n",
    "\n",
    "- **Score_time_mean**   \n",
    "  Le temps moyen pour faire des prédictions est aussi très faible, ce qui est normal pour ce type de modèle.\n",
    "\n",
    "Ces résultats montrent que le `Dummy Classifier` est utile comme point de référence. Un modèle plus sophistiqué devrait idéalement obtenir une AUC plus élevée et être capable de mieux distinguer entre les classes. Les prochaines étapes consisteront probablement à comparer ces résultats avec ceux des modèles plus complexes pour évaluer leur performance relative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89946b3a-1cca-445f-88b4-392735fb639c",
   "metadata": {},
   "source": [
    "## 2.2 - Régression logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc41d26-472e-4044-8810-30997a7cb22b",
   "metadata": {},
   "source": [
    "La régression logistique est un modèle statistique utilisé principalement pour des tâches de classification binaire. Contrairement à une régression linéaire qui prédit une valeur continue, la régression logistique prédit la probabilité qu'un échantillon appartienne à une classe donnée. En sortie, elle donne des probabilités comprises entre 0 et 1, qui peuvent être ensuite converties en classes (0 ou 1) en fonction d'un seuil.\n",
    "\n",
    "En tant que premier modèle de base, la régression logistique nous permettra d'établir une baseline pour prédire la probabilité de défaut de paiement. En testant le modèle avant et après l'application de SMOTE, nous allons comparer l'impact du rééquilibrage des classes sur les performances du modèle.\n",
    "L'application de SMOTE avant l'entraînement d'une régression logistique peut potentiellement améliorer les performances, notamment sur des données déséquilibrées, car elle crée de nouveaux échantillons dans la classe minoritaire."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d051e404-5a8f-4801-aa00-e8a047301cbb",
   "metadata": {},
   "source": [
    "# On va définir le/les modèle(s)\n",
    "models = {\n",
    "    'Régression Logistique': LogisticRegression(max_iter=1000, n_jobs=-1, random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d824a0-3737-4134-8750-1e30919d6fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On va définir le/les modèle(s)\n",
    "models = {\n",
    "    'Régression Logistique': LogisticRegression(max_iter=1000, n_jobs=-1, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42, n_jobs=-1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d45885-4953-4abc-b373-48e9b48ef1f0",
   "metadata": {},
   "source": [
    "Le paramètre `n_jobs` est couramment utilisé dans scikit-learn pour spécifier combien de cœurs de processeur doivent être utilisés pour exécuter les opérations en parallèle. Voici un guide sur comment utiliser `n_jobs` pour accélérer l'entraînement de vos modèles :\n",
    "\n",
    "- **`n_jobs=-1`** : Utilise **tous** les cœurs disponibles sur votre machine. Cela maximise la parallélisation et peut réduire le temps d'entraînement, surtout si vous travaillez avec de grands ensembles de données.\n",
    "- **`n_jobs=1`** (ou ne pas spécifier) : Utilise un seul cœur (c'est le comportement par défaut).\n",
    "- **`n_jobs=N`** : Utilise exactement **N** cœurs de processeur. Par exemple, `n_jobs=4` utilisera 4 cœurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3095bf-172d-4190-a482-dfeeec385636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On va entraîner et évaluer le modèle via cross validation sur le train_set\n",
    "evaluate_models_with_cross_validation(models, X_train_split, y_train_split, log_to_mlflow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3214bf-87ef-42c9-ad52-b81d3ab95388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On va évaluer le modèle sur le test_set et analyser les graphiques\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Évaluation du modèle: {model_name}\")\n",
    "    evaluate_model(model, X_train_split, y_train_split, X_val, y_val, model_name, retrain=True, log_to_mlflow=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcb52b7-3df7-4e2c-9f20-15829dbe2bf2",
   "metadata": {},
   "source": [
    "Les résultats de la régression logistique avant l'application de SMOTE montrent des performances satisfaisantes mais trompeuses :\n",
    "\n",
    "- **AUC_mean** : 0.75 — Cela signifie que le modèle parvient à différencier les classes positives et négatives avec une bonne précision.\n",
    "- **Accuracy** : 92 % — Le modèle est capable de prédire correctement environ 92 % des exemples.\n",
    "- **Matrice de confusion** :\n",
    "  - **True Negative** : Le modèle prédit correctement que 84 730 échantillons n'appartiennent pas à la classe positive.\n",
    "  - **True Positive** : Seulement 159 échantillons de la classe positive sont correctement prédits.\n",
    "  - **False Negative** : Il y a un nombre significatif d'échantillons de la classe positive qui sont incorrectement classés comme négatifs.\n",
    "\n",
    "Nous pouvons analyser cela de la manière suivante :\n",
    "- **AUC** : le modèle parvient à bien capturer les relations dans les données, mais il pourrait encore être amélioré.\n",
    "- **Matrice de confusion** : Le nombre élevé de faux négatifs par rapport au nombre relativement faible de vrais positifs est un signe que le modèle a du mal à identifier correctement la classe minoritaire (les défauts de paiement).\n",
    "\n",
    "Après SMOTE, nous pourrons comparer ces résultats pour évaluer si le rééquilibrage des classes améliore la capacité du modèle à prédire la classe minoritaire. Il serait intéressant de voir si le nombre de vrais positifs augmente après SMOTE, tout en maintenant un bon compromis avec le nombre de faux positifs et la précision globale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae05ffe-35d8-402d-9546-3671cf8dc4be",
   "metadata": {},
   "source": [
    "## 2.3 - Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6794a3-fb79-4a61-8732-b72456052126",
   "metadata": {},
   "source": [
    "Le **Gradient Boosting** est une technique de machine learning supervisée utilisée principalement pour les tâches de classification et de régression. Il repose sur l'idée de combiner plusieurs modèles simples (généralement des arbres de décision) pour former un modèle plus performant. Le principe clé du Gradient Boosting est de construire ces modèles séquentiellement, où chaque modèle essaie de corriger les erreurs commises par les modèles précédents. \n",
    "\n",
    "Le modèle final est donc une combinaison pondérée de ces modèles faibles, où les modèles successifs \"boostent\" les performances des précédents en se concentrant sur les observations mal prédictes.\n",
    "\n",
    "Le **Gradient Boosting** peut être coûteux en termes de calcul et de temps d'entraînement, car il construit les arbres de manière séquentielle. Pour répondre à ce problème, deux algorithmes populaires, **XGBoost** et **LGBM**, ont été développés pour optimiser cette approche.\n",
    "\n",
    "1. **XGBoost (Extreme Gradient Boosting)**\n",
    "\n",
    "**XGBoost** est une implémentation optimisée de l'algorithme de Gradient Boosting qui est particulièrement efficace pour les grandes bases de données. Il se distingue par sa rapidité et son efficacité, grâce à plusieurs optimisations importantes :\n",
    "- **Traitement parallèle** : Contrairement au Gradient Boosting classique qui construit les arbres séquentiellement, XGBoost permet d'entraîner les arbres en parallèle, ce qui réduit considérablement le temps d'exécution.\n",
    "- **Régularisation** : XGBoost intègre des termes de régularisation (L1 et L2) pour éviter le surapprentissage, en forçant le modèle à être plus simple.\n",
    "- **Gestion des valeurs manquantes** : XGBoost gère naturellement les données manquantes en apprenant des chemins optimaux pour les valeurs manquantes dans les arbres de décision.\n",
    "\n",
    "\n",
    "2. **LGBM (LightGBM)**\n",
    "\n",
    "**LightGBM** est une autre implémentation optimisée du Gradient Boosting, développée par Microsoft. **LGBM** se distingue par sa capacité à traiter de très grands volumes de données de manière encore plus rapide que XGBoost grâce à une approche de construction d'arbres appelée **Leaf-wise Tree Growth** :\n",
    "- **Growth Leaf-Wise** : Contrairement à XGBoost qui ajoute des niveaux à chaque arbre (Level-wise), LGBM choisit d'ajouter des feuilles uniquement aux branches qui réduisent le plus l'erreur. Cela permet d'obtenir des arbres plus profonds et plus efficaces sans augmenter significativement le coût de calcul.\n",
    "- **Gestion des grandes bases de données** : LGBM est particulièrement efficace pour des datasets volumineux ou très dimensionnels.\n",
    "- **Optimisation de la mémoire** : LGBM a été conçu pour être plus économe en mémoire que XGBoost, ce qui est un avantage pour des tâches de grande échelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a306ea36-7d5b-4dcc-b0d5-bfaf3843ed0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On va définir le/les modèle(s)\n",
    "models = {\n",
    "    'LGBM Classifier': lgb.LGBMClassifier(random_state=42, device='gpu'),\n",
    "    'XGBoost': xgb.XGBClassifier(random_state=42, tree_method='gpu_hist')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d145fde8-73d0-41bd-a320-26137fdac9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification GPU (XGBoost)\n",
    "print(f\"XGBoost GPU status: {xgb.XGBClassifier(tree_method='gpu_hist').get_xgb_params()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cd93a8-b9dd-44c9-9aab-2da56447e92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyage des noms de colonnes en supprimant les caractères spéciaux\n",
    "X_train_split.columns = X_train_split.columns.str.replace(r'[^\\w]', '_', regex=True)\n",
    "#X_train_split_smote.columns = X_train_split_smote.columns.str.replace(r'[^\\w]', '_', regex=True)\n",
    "X_val.columns = X_val.columns.str.replace(r'[^\\w]', '_', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b202413-01ec-4a07-91ae-a0c895d91220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On va entraîner et évaluer le modèle via cross validation sur le train_set\n",
    "evaluate_models_with_cross_validation(models, X_train_split, y_train_split, log_to_mlflow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a99631b-eaa2-4c37-8453-4a8bdf02bdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On va évaluer le modèle sur le test_set et analyser les graphiques\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Évaluation du modèle: {model_name}\")\n",
    "    evaluate_model(model, X_train_split, y_train_split, X_val, y_val, model_name, retrain=True, log_to_mlflow=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de7f329-84d2-41a7-bb9e-ca588a364509",
   "metadata": {},
   "source": [
    "Voici une analyse rapide des résultats obtenus avant l'application de **SMOTE** pour les deux modèles **LGBM Classifier** et **XGBoost** :\n",
    "\n",
    "**LGBM Classifier :**\n",
    "- **AUC_mean** : 0.77 – Un score AUC qui montre une bonne capacité de discrimination. Cela indique que le modèle parvient à bien différencier entre les classes positives (clients en défaut) et négatives (clients non en défaut).\n",
    "- **Accuracy_mean** : 0.91 – Le modèle atteint une très bonne précision, ce qui reflète la proportion de prédictions correctes.\n",
    "- **Fit_time_mean** : 8.98 secondes – L'entraînement du modèle est assez rapide.\n",
    "- **Score_time_mean** : 0.1820 seconde – Le temps pour réaliser les prédictions est également très court, ce qui est un atout pour les systèmes de scoring en temps réel.\n",
    "\n",
    "**XGBoost :**\n",
    "- **AUC_mean** : 0.76 – Légèrement inférieur à celui de LGBM, mais reste un score solide qui montre une bonne capacité de discrimination.\n",
    "- **Accuracy_mean** : 0.91 – Un peu plus bas que LGBM, mais reste très élevé et performant.\n",
    "- **Fit_time_mean** : 13.12 secondes – L'entraînement de XGBoost est un peu plus long que celui de LGBM, ce qui est habituel étant donné l'approche plus intensive de XGBoost.\n",
    "- **Score_time_mean** : 0.4146 seconde – Le temps de prédiction est environ deux fois plus long que LGBM, mais reste raisonnable pour des systèmes de production.\n",
    "\n",
    "**Comparaison globale :**\n",
    "- **Performance** : LGBM affiche de meilleures performances en termes de score AUC et de précision, bien que les deux modèles soient assez proches.\n",
    "- **Temps d'exécution** : LGBM est plus rapide à l'entraînement et pour la prédiction, ce qui peut en faire un candidat plus efficace dans des contextes où la rapidité est cruciale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c91e702-923f-4682-83ab-02bab822d3aa",
   "metadata": {},
   "source": [
    "# 3 - Deuxième modélisation : après SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf513781-5fdb-49fa-8e4e-21075ff3446b",
   "metadata": {},
   "source": [
    "L'utilisation de **SMOTE** (*Synthetic Minority Over-sampling Technique*) est particulièrement utile lorsque l'on travaille avec des ensembles de données déséquilibrés. En data science, un ensemble de données est dit déséquilibré lorsque la répartition des classes dans la variable cible n'est pas équilibrée, c'est-à-dire que l'une des classes (par exemple, les cas de défaut de paiement) est beaucoup moins représentée que l'autre (les non-défauts).\n",
    "\n",
    "**Pourquoi utiliser SMOTE ?**\n",
    "\n",
    "1. **Déséquilibre des classes** : Lorsque les classes sont déséquilibrées, les algorithmes de machine learning ont tendance à biaiser leurs prédictions en faveur de la classe majoritaire. Par exemple, dans le cas d’un modèle de scoring de crédit, si les défauts de paiement (classe minoritaire) représentent seulement 5 % des observations, un modèle pourrait simplement prédire que tout le monde remboursera son prêt pour obtenir une haute précision (en ignorant totalement les 5 % de défauts).\n",
    "   \n",
    "2. **SMOTE pour rééquilibrer les classes** : SMOTE génère de nouvelles observations synthétiques pour la classe minoritaire en combinant plusieurs exemples existants proches dans l’espace des caractéristiques. Cela permet de rééquilibrer le jeu de données sans simplement dupliquer les observations, ce qui pourrait mener à de l'overfitting.\n",
    "\n",
    "3. **Amélioration des performances du modèle** :\n",
    "   - **Meilleure sensibilité aux classes minoritaires** : Grâce à SMOTE, notre modèle pourra mieux identifier les cas appartenant à la classe minoritaire (dans notre cas, les clients à risque de défaut de paiement).\n",
    "   - **Précision des prédictions** : Cela évite au modèle de donner une précision trompeuse en prédisant principalement la classe majoritaire. L'utilisation de métriques comme l'**AUC-ROC** ou le **F1-Score** devient également plus pertinente car elles tiennent compte des performances sur les deux classes.\n",
    "   \n",
    "4. **Limitation du biais du modèle** : Sans une correction comme SMOTE, le modèle risque de négliger la classe minoritaire et de mal gérer les faux négatifs (prédire qu'un client remboursera alors qu'il va faire défaut). Dans le contexte d'un modèle de scoring crédit, cela peut avoir des conséquences importantes sur le coût métier.\n",
    "\n",
    "**Comment fonctionne SMOTE ?**\n",
    "\n",
    "SMOTE fonctionne en trois étapes :\n",
    "1. **Sélection des points de la classe minoritaire**.\n",
    "2. **Création des observations synthétiques** : Pour chaque point de la classe minoritaire, SMOTE sélectionne les k plus proches voisins (en général 5). Ensuite, il génère un point synthétique sur la ligne qui relie l'exemple de la classe minoritaire sélectionné et l'un de ses voisins.\n",
    "3. **Ajout des nouvelles données au jeu d'entraînement** : Les observations synthétiques sont ajoutées au jeu d'entraînement, augmentant la proportion de la classe minoritaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c39331a-63a1-4f8e-aaae-fb1fe0416008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On va commencer par initialiser notre expérimentation MLFlow\n",
    "mlflow.set_experiment(\"2 - Deuxième modélisation : après SMOTE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fa2b15-4126-4c46-8e20-33c06208eb37",
   "metadata": {},
   "source": [
    "## 3.1 - Application du SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718b0689-450e-47a5-ad56-03011c8c9b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avant SMOTE\n",
    "print(\"Répartition des classes avant SMOTE :\")\n",
    "print(pd.Series(y_train_split).value_counts())\n",
    "\n",
    "# Application de SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_split_smote, y_train_split_smote = smote.fit_resample(X_train_split, y_train_split)\n",
    "\n",
    "# Après SMOTE\n",
    "print(\"\\nRépartition des classes après SMOTE :\")\n",
    "print(pd.Series(y_train_split_smote).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4915939-2f76-418b-8501-5dbac3d81eee",
   "metadata": {},
   "source": [
    "## 3.2 - Entraînement des modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93310803-dc90-4d21-b657-8f7fef11a782",
   "metadata": {},
   "source": [
    "Nous allons tester les 4 modèles précédents après application de SMOTE puis comparer nos résultats.\n",
    "\n",
    "Tester un Dummy Classifier après avoir appliqué SMOTE peut être intéressant dans certains cas, mais il faut garder à l’esprit que cela n'a pas de réel sens pratique dans un projet de modélisation. Pourquoi ?\n",
    "\n",
    "- **SMOTE** est une méthode utilisée pour équilibrer les classes dans les jeux de données déséquilibrés en générant des échantillons synthétiques pour la classe minoritaire.\n",
    "- Le **Dummy Classifier**, en revanche, est un modèle très basique qui fait des prédictions triviales (par exemple, prédire la classe majoritaire ou une prédiction aléatoire).\n",
    "\n",
    "Ce qui pourrait arriver ?\n",
    "- Après SMOTE, notre jeu de données d'entraînement sera équilibré, mais un Dummy Classifier qui utilise la stratégie \"Most Frequent\" ne fera que prédire les classes en fonction de la distribution originale. Si les classes sont équilibrées, ce classifieur pourrait prédire aléatoirement.\n",
    "- **Résultat attendu** : Le Dummy Classifier après SMOTE pourrait avoir un score AUC ou une accuracy proches de 0.5, car il ferait des prédictions quasi-aléatoires dans un contexte où les classes sont équilibrées.\n",
    "\n",
    "Pourquoi le tester ?\n",
    "Tester un Dummy Classifier après SMOTE permet d’avoir un repère pour vérifier si les modèles plus avancés (comme ceux basés sur des algorithmes de boosting ou de régression logistique) apportent une réelle valeur ajoutée par rapport à un modèle trivial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d6b7e0-ad5e-4790-a269-75021e3aab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On va définir le/les modèle(s)\n",
    "models = {\n",
    "    'Dummy Classifier': DummyClassifier(strategy='most_frequent'),\n",
    "    'Régression Logistique': LogisticRegression(random_state=42, max_iter=1000, n_jobs=-1, ),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42, n_jobs=-1),\n",
    "    'LGBM Classifier': lgb.LGBMClassifier(random_state=42, device='gpu'),\n",
    "    'XGBoost': xgb.XGBClassifier(random_state=42, tree_method='gpu_hist')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f528fdf-1319-4cf7-92d9-51c75bf7bbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On va entraîner et évaluer le modèle via cross validation sur le train_set\n",
    "evaluate_models_with_cross_validation(models, X_train_split_smote, y_train_split_smote, log_to_mlflow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd7472d-ce9c-4e97-9a80-ca1d325a557d",
   "metadata": {},
   "source": [
    "## 3.3 - Evaluation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e310a23-f54c-4da2-a892-84362c776e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On va évaluer le modèle sur le test_set et analyser les graphiques\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Évaluation du modèle: {model_name}\")\n",
    "    evaluate_model(model, X_train_split_smote, y_train_split_smote, X_val, y_val, model_name, retrain=True, log_to_mlflow=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ceb8fb-ab95-423d-9658-020231744675",
   "metadata": {},
   "source": [
    "Les résultats que nousu obtenons pour le **Dummy Classifier** après avoir appliqué SMOTE montrent un écart important entre l'accuracy moyenne pendant l'entraînement et l'accuracy sur le jeu de test. \n",
    "\n",
    "1. **Accuracy élevée sur le jeu de test** :\n",
    "   - L'accuracy suggère que le modèle prédit la classe majoritaire (negative) pour la quasi-totalité des échantillons du jeu de test. \n",
    "   - Cette accuracy élevée est trompeuse car le modèle ne capture aucune information utile, comme le montre la **matrice de confusion** : toutes les prédictions sont dans la classe \"Predicted Negative\".\n",
    "   - **Pourquoi ?** Le Dummy Classifier avec SMOTE prédit probablement la classe majoritaire (ou aléatoirement), mais le jeu de test reste déséquilibré, donc la majorité des échantillons appartiennent à la classe majoritaire. C'est pourquoi il semble obtenir une bonne accuracy.\n",
    "\n",
    "2. **Matrice de confusion** :\n",
    "   - La matrice de confusion montre clairement que le Dummy Classifier ne prédit jamais la classe positive (aucune prédiction correcte pour les `True Positive`).\n",
    "   - Cela confirme que le modèle est trivial et n’apporte aucune valeur dans ce contexte, malgré une bonne accuracy apparente.\n",
    "\n",
    "\n",
    "Ces résultats confirment que **le Dummy Classifier ne tire pas profit du rééquilibrage SMOTE**, et bien que l'accuracy puisse sembler bonne sur le jeu de test, elle n’est pas un indicateur pertinent ici. Il est important de se concentrer sur d’autres métriques comme le **AUC-ROC** ou la **précision sur la classe minoritaire** pour mieux évaluer les modèles dans des situations de classes déséquilibrées."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61d80e8-21a9-4e32-a754-82a07778dd54",
   "metadata": {},
   "source": [
    "Les résultats de la **régression logistique** avant l'application de SMOTE montrent des performances satisfaisantes mais trompeuses :\n",
    "\n",
    "- **AUC_mean** : 0.75 — Cela signifie que le modèle parvient à différencier les classes positives et négatives avec une bonne précision.\n",
    "- **Accuracy** : 92 % — Le modèle est capable de prédire correctement environ 92 % des exemples.\n",
    "- **Matrice de confusion** :\n",
    "  - **True Negative** : Le modèle prédit correctement que 84 730 échantillons n'appartiennent pas à la classe positive.\n",
    "  - **True Positive** : Seulement 159 échantillons de la classe positive sont correctement prédits.\n",
    "  - **False Negative** : Il y a un nombre significatif d'échantillons de la classe positive qui sont incorrectement classés comme négatifs.\n",
    "\n",
    "Nous pouvons analyser cela de la manière suivante :\n",
    "- **AUC** : le modèle parvient à bien capturer les relations dans les données, mais il pourrait encore être amélioré.\n",
    "- **Matrice de confusion** : Le nombre élevé de faux négatifs par rapport au nombre relativement faible de vrais positifs est un signe que le modèle a du mal à identifier correctement la classe minoritaire (les défauts de paiement).\n",
    "\n",
    "Après SMOTE, nous pourrons comparer ces résultats pour évaluer si le rééquilibrage des classes améliore la capacité du modèle à prédire la classe minoritaire. Il serait intéressant de voir si le nombre de vrais positifs augmente après SMOTE, tout en maintenant un bon compromis avec le nombre de faux positifs et la précision globale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e30ead-9afd-46b9-9ff8-2e2fe5fe2ff6",
   "metadata": {},
   "source": [
    "Voici une analyse rapide des résultats obtenus avant l'application de **SMOTE** pour les deux modèles **LGBM Classifier** et **XGBoost** :\n",
    "\n",
    "**LGBM Classifier :**\n",
    "- **AUC_mean** : 0.7731 – Un score AUC qui montre une bonne capacité de discrimination. Cela indique que le modèle parvient à bien différencier entre les classes positives (clients en défaut) et négatives (clients non en défaut).\n",
    "- **Accuracy_mean** : 0.9195 – Le modèle atteint une très bonne précision, ce qui reflète la proportion de prédictions correctes.\n",
    "- **Fit_time_mean** : 8.98 secondes – L'entraînement du modèle est assez rapide.\n",
    "- **Score_time_mean** : 0.1820 seconde – Le temps pour réaliser les prédictions est également très court, ce qui est un atout pour les systèmes de scoring en temps réel.\n",
    "\n",
    "**XGBoost :**\n",
    "- **AUC_mean** : 0.7642 – Légèrement inférieur à celui de LGBM, mais reste un score solide qui montre une bonne capacité de discrimination.\n",
    "- **Accuracy_mean** : 0.9180 – Un peu plus bas que LGBM, mais reste très élevé et performant.\n",
    "- **Fit_time_mean** : 13.12 secondes – L'entraînement de XGBoost est un peu plus long que celui de LGBM, ce qui est habituel étant donné l'approche plus intensive de XGBoost.\n",
    "- **Score_time_mean** : 0.4146 seconde – Le temps de prédiction est environ deux fois plus long que LGBM, mais reste raisonnable pour des systèmes de production.\n",
    "\n",
    "**Comparaison globale :**\n",
    "- **Performance** : LGBM affiche de meilleures performances en termes de score AUC et de précision, bien que les deux modèles soient assez proches.\n",
    "- **Temps d'exécution** : LGBM est plus rapide à l'entraînement et pour la prédiction, ce qui peut en faire un candidat plus efficace dans des contextes où la rapidité est cruciale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f254957-b63c-43f9-ac80-5aa66d0254fa",
   "metadata": {},
   "source": [
    "# 4 - Recherche des meilleurs hyperparamètres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4b8182-e779-4f21-b3f1-5015f32dba75",
   "metadata": {},
   "source": [
    "L'objectif principal maintenant est d'optimiser les performances des modèles choisis via le **réglage des hyperparamètres**. Pour ce faire, il serait plus efficace de procéder à une **recherche d'hyperparamètres (Grid Search ou Random Search)** combinée avec une validation croisée.\n",
    "\n",
    "- **Pourquoi faire une validation croisée ici ?**\n",
    "  - La validation croisée (CV) lors du réglage des hyperparamètres garantit l'évaluation des performances du modèle sur différentes portions du dataset, limitant ainsi les risques de surapprentissage (overfitting) sur une seule partition des données.\n",
    "  - Cela est surtout important pour éviter que le modèle soit trop optimisé pour une seule division du train-test split.\n",
    "\n",
    "Une fois que nous aurons les **meilleurs hyperparamètres** pour notre modèle, nous pourrons passer à l'étape d'élaboration du **score métier**. Cette étape peut se faire après le réglage des hyperparamètres et sur une seule division de train-test (sans validation croisée répétée), car nous nous focaliserons sur l'évaluation des **coûts associés aux erreurs de prédiction** (faux positifs et faux négatifs), en fonction des besoins spécifiques du métier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe8c172-91f9-4a9a-a98c-1bca27ebc460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On va commencer par initialiser notre expérimentation MLFlow\n",
    "mlflow.set_experiment(\"3 - Hyperparameter Tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124e2156-0347-4c3b-adf2-dd76d994d95d",
   "metadata": {},
   "source": [
    "## 4.1 - LGBM Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c13f739-7825-4806-807b-1cfaeb8ecfc0",
   "metadata": {},
   "source": [
    "Dans cette section, nous procédons à l'optimisation des hyperparamètres de notre modèle de LightGBM afin d'améliorer ses performances. L'optimisation des hyperparamètres est une étape importante dans le développement d'un modèle machine learning, car elle permet d'ajuster les paramètres du modèle pour maximiser les performances sur un ensemble de validation.\n",
    "\n",
    "Pour ce faire, nous avons utilisé une méthode de **RandomizedSearchCV**, qui permet d'explorer un ensemble prédéfini d'hyperparamètres de manière aléatoire, tout en effectuant une validation croisée (cross-validation) sur chaque combinaison d'hyperparamètres. Cela permet de limiter le temps de calcul tout en cherchant les meilleures combinaisons de paramètres.\n",
    "\n",
    "L'objectif principal est d'optimiser des hyperparamètres tels que :\n",
    "- `num_leaves` : contrôle la complexité de l'arbre.\n",
    "- `max_depth` : limite la profondeur de chaque arbre.\n",
    "- `learning_rate` : détermine la vitesse d'apprentissage du modèle.\n",
    "- `n_estimators` : contrôle le nombre d'arbres.\n",
    "- `min_child_samples` : spécifie le nombre minimum d'échantillons dans une feuille.\n",
    "\n",
    "À l'aide de cette approche, nous cherchons à maximiser le score AUC, une métrique clé pour les problèmes de classification déséquilibrés. Après cette étape, nous utiliserons les meilleurs hyperparamètres pour ajuster notre modèle final et procéder à l'évaluation sur les données de test."
   ]
  },
  {
   "cell_type": "raw",
   "id": "6c6853a2-6713-486b-b0c2-a330cafc6ea5",
   "metadata": {},
   "source": [
    "# Lancement du tracking avec MLFlow\n",
    "with mlflow.start_run(run_name=\"LGBM Hyperparameter Tuning\"):\n",
    "    \n",
    "    # Grille d'hyperparamètres à tester\n",
    "    param_grid = {\n",
    "        'num_leaves': [31, 50, 100],\n",
    "        'max_depth': [-1, 5, 10],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'min_child_samples': [20, 30, 50]\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMClassifier(random_state=42, device='gpu')\n",
    "\n",
    "    # Initialisation de RandomizedSearchCV\n",
    "    random_search = RandomizedSearchCV(estimator=model, \n",
    "                                       param_distributions=param_grid, \n",
    "                                       scoring={'roc_auc': 'roc_auc', 'accuracy': 'accuracy'},\n",
    "                                       refit='roc_auc',\n",
    "                                       cv=5, \n",
    "                                       n_iter=50,  # Nombre d'itérations à tester\n",
    "                                       n_jobs=-1, \n",
    "                                       verbose=3, \n",
    "                                       random_state=42)\n",
    "\n",
    "    # Mesure du temps de recherche des hyperparamètres\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Ajustement sur l'ensemble d'entraînement\n",
    "    random_search.fit(X_train_split, y_train_split)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calcul du temps écoulé\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    # Résultats\n",
    "    print(\"Temps pris pour trouver les hyperparamètres :\", elapsed_time, \"secondes\")\n",
    "    print(\"Meilleurs paramètres :\", random_search.best_params_)\n",
    "    print(\"Meilleur score AUC :\", random_search.best_score_)\n",
    "\n",
    "    # Modèle optimisé\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    # Enregistrement des hyperparamètres et des métriques dans MLFlow\n",
    "    mlflow.log_params(random_search.best_params_)  # Log des meilleurs hyperparamètres\n",
    "    mlflow.log_metric(\"best_auc\", random_search.best_score_)  # Log du meilleur score AUC\n",
    "    mlflow.log_metric(\"search_time_seconds\", elapsed_time)  # Log du temps pris\n",
    "\n",
    "    # Générer la signature du modèle (entrées/sorties)\n",
    "    signature = infer_signature(X_train_split, best_model.predict(X_train_split))\n",
    "\n",
    "     # Enregistrement du modèle dans MLFlow et Model Registry avec versioning\n",
    "    mlflow.lightgbm.log_model(\n",
    "        best_model,\n",
    "        artifact_path=\"best_model_lgbm\",\n",
    "        signature=signature,\n",
    "        registered_model_name=\"Best_model_lgbm-classification\"  # Enregistrement avec nom et versioning\n",
    "    )\n",
    "\n",
    "    # Enregistrement du modèle optimisé dans MLFlow\n",
    "    #mlflow.lightgbm.log_model(best_model, \"best_model_lgbm\", signature=signature)\n",
    "\n",
    "    # Terminer le run\n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a6e46f49-08c2-49db-8bb6-b187a1304c52",
   "metadata": {},
   "source": [
    "# Grille d'hyperparamètres à tester\n",
    "param_grid = {\n",
    "    'num_leaves': [31, 50, 100],\n",
    "    'max_depth': [-1, 5, 10],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'min_child_samples': [20, 30, 50]\n",
    "}\n",
    "\n",
    "model = lgb.LGBMClassifier(random_state=42, device='gpu')\n",
    "\n",
    "# Initialisation de RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(estimator=model, \n",
    "                                   param_distributions=param_grid, \n",
    "                                   scoring={'roc_auc': 'roc_auc', 'accuracy': 'accuracy'},\n",
    "                                   refit='roc_auc',\n",
    "                                   cv=5, \n",
    "                                   n_iter=50,  # Nombre d'itérations à tester\n",
    "                                   n_jobs=-1, \n",
    "                                   verbose=3, \n",
    "                                   random_state=42)\n",
    "\n",
    "# Mesure du temps de recherche des hyperparamètres\n",
    "start_time = time.time()\n",
    "\n",
    "# Ajustement sur l'ensemble d'entraînement\n",
    "random_search.fit(X_train_split, y_train_split)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Calcul du temps écoulé\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Temps pris pour trouver les hyperparamètres :\", elapsed_time, \"secondes\")\n",
    "\n",
    "# Lancer le tracking global avec MLFlow (pour le meilleur modèle)\n",
    "with mlflow.start_run(run_name=\"LGBM CLassifier\"):\n",
    "\n",
    "    # Parcourir tous les résultats de RandomizedSearchCV\n",
    "    for i in range(len(random_search.cv_results_['params'])):\n",
    "        # Démarrer un run MLflow pour chaque ensemble d'hyperparamètres\n",
    "        with mlflow.start_run(nested=True):  # Run imbriqué sous la run principale\n",
    "\n",
    "            # Récupérer les paramètres et les résultats pour cette itération\n",
    "            params = random_search.cv_results_['params'][i]\n",
    "            mean_test_auc = random_search.cv_results_['mean_test_roc_auc'][i]\n",
    "            mean_test_accuracy = random_search.cv_results_['mean_test_accuracy'][i]\n",
    "            fit_time = random_search.cv_results_['mean_fit_time'][i]\n",
    "            score_time = random_search.cv_results_['mean_score_time'][i]\n",
    "\n",
    "            # Enregistrer les hyperparamètres et les métriques dans MLflow\n",
    "            mlflow.log_params(params)\n",
    "            mlflow.log_metric(\"mean_test_auc\", mean_test_auc)\n",
    "            mlflow.log_metric(\"mean_test_accuracy\", mean_test_accuracy)\n",
    "            mlflow.log_metric(\"fit_time\", fit_time)\n",
    "            mlflow.log_metric(\"score_time\", score_time)\n",
    "\n",
    "    # Enregistrement du meilleur modèle après la recherche\n",
    "    best_model = random_search.best_estimator_\n",
    "    mlflow.log_params(random_search.best_params_)\n",
    "    mlflow.log_metric(\"best_auc\", random_search.best_score_)\n",
    "    mlflow.log_metric(\"search_time_seconds\", elapsed_time)\n",
    "\n",
    "    # Générer la signature du modèle (entrées/sorties)\n",
    "    signature = infer_signature(X_train_split, best_model.predict(X_train_split))\n",
    "\n",
    "    # Enregistrer le meilleur modèle dans MLflow\n",
    "    mlflow.lightgbm.log_model(\n",
    "        best_model,\n",
    "        artifact_path=\"best_model_lgbm\",\n",
    "        signature=signature,\n",
    "        registered_model_name=\"Best_model_lgbm-classification\"\n",
    "    )\n",
    "\n",
    "    print(\"Meilleurs paramètres :\", random_search.best_params_)\n",
    "    print(\"Meilleur score AUC :\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2763aaf7-8f7a-435c-9660-ed9d5dd6399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grille d'hyperparamètres à tester\n",
    "param_grid = {\n",
    "    'num_leaves': [31, 50, 100],\n",
    "    'max_depth': [-1, 5, 10],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'min_child_samples': [20, 30, 50]\n",
    "}\n",
    "\n",
    "model = lgb.LGBMClassifier(random_state=42, device='gpu')\n",
    "\n",
    "# Initialisation de RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(estimator=model, \n",
    "                                   param_distributions=param_grid, \n",
    "                                   scoring={'roc_auc': 'roc_auc', 'accuracy': 'accuracy'},\n",
    "                                   refit='roc_auc',\n",
    "                                   cv=5, \n",
    "                                   n_iter=50,  # Nombre d'itérations à tester\n",
    "                                   n_jobs=-1, \n",
    "                                   verbose=3, \n",
    "                                   random_state=42)\n",
    "\n",
    "# Mesure du temps de recherche des hyperparamètres\n",
    "start_time = time.time()\n",
    "\n",
    "# Ajustement sur l'ensemble d'entraînement\n",
    "random_search.fit(X_train_split, y_train_split)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Calcul du temps écoulé\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Temps pris pour trouver les hyperparamètres :\", elapsed_time, \"secondes\")\n",
    "\n",
    "# Lancer le tracking global avec MLFlow (pour le meilleur modèle)\n",
    "with mlflow.start_run(run_name=\"LGBM Classifier\"):\n",
    "\n",
    "    # Parcourir tous les résultats de RandomizedSearchCV\n",
    "    for i in range(len(random_search.cv_results_['params'])):\n",
    "        # Démarrer un run MLflow pour chaque ensemble d'hyperparamètres\n",
    "        with mlflow.start_run(nested=True):  # Run imbriqué sous la run principale\n",
    "\n",
    "            # Récupérer les paramètres et les résultats pour cette itération\n",
    "            params = random_search.cv_results_['params'][i]\n",
    "            mean_test_auc = random_search.cv_results_['mean_test_roc_auc'][i]\n",
    "            mean_test_accuracy = random_search.cv_results_['mean_test_accuracy'][i]\n",
    "            fit_time = random_search.cv_results_['mean_fit_time'][i]\n",
    "            score_time = random_search.cv_results_['mean_score_time'][i]\n",
    "\n",
    "            # Enregistrer les hyperparamètres et les métriques dans MLflow\n",
    "            mlflow.log_param(\"Model\", \"LGBM Classifier\")\n",
    "            mlflow.log_params(params)\n",
    "            mlflow.log_metric(\"AUC\", mean_test_auc)\n",
    "            mlflow.log_metric(\"Accuracy\", mean_test_accuracy)\n",
    "            mlflow.log_metric(\"Fit_time_mean\", fit_time)\n",
    "            mlflow.log_metric(\"Score_time_mean\", score_time)\n",
    "\n",
    "    # Enregistrement du meilleur modèle après la recherche\n",
    "    mlflow.log_param(\"Model\", \"LGBM Classifier\")\n",
    "    best_model_lgbm = random_search.best_estimator_\n",
    "    mlflow.log_params(random_search.best_params_)\n",
    "    mlflow.log_metric(\"AUC\", random_search.best_score_)\n",
    "    best_index = random_search.best_index_\n",
    "    best_accuracy = random_search.cv_results_['mean_test_accuracy'][best_index]\n",
    "    mlflow.log_metric(\"Accuracy\", best_accuracy)\n",
    "    mlflow.log_metric(\"search_time_seconds\", elapsed_time)\n",
    "\n",
    "    # Générer la signature du modèle (entrées/sorties)\n",
    "    signature = infer_signature(X_train_split, best_model_lgbm.predict(X_train_split))\n",
    "\n",
    "    # Enregistrer le meilleur modèle dans MLflow\n",
    "    mlflow.lightgbm.log_model(\n",
    "        best_model_lgbm,\n",
    "        artifact_path=\"LGBM Classifier\",\n",
    "        signature=signature,\n",
    "        registered_model_name=\"LGBM Classifier-classification-model\"\n",
    "    )\n",
    "\n",
    "    print(\"Meilleurs paramètres :\", random_search.best_params_)\n",
    "    print(\"Meilleur score AUC :\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1278ee72-35be-430b-b779-dc404f273784",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "41af8edc-ed5d-4a59-8cb0-88d1bb1f4113",
   "metadata": {},
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "client = MlflowClient()\n",
    "\n",
    "# Promouvoir la version 4 à production\n",
    "client.transition_model_version_stage(\n",
    "    name=\"LGBM Classifier-classification-model\",\n",
    "    version=4,\n",
    "    stage=\"production\"\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "399939a7-1eab-438d-967b-86daffc1d6b6",
   "metadata": {},
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "client = MlflowClient()\n",
    "for mv in client.search_model_versions(\"name='LGBM Classifier-classification-model'\"):\n",
    "    print(mv.version, mv.current_stage, mv.run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989e999f-b84d-4819-8ded-afe7795aec9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "decf8738-a158-476e-bf8c-e842fab1e162",
   "metadata": {},
   "source": [
    "## 4.2 - XGBoost Classifier"
   ]
  },
  {
   "cell_type": "raw",
   "id": "131c263c-be43-447a-9429-ca4a01938a38",
   "metadata": {},
   "source": [
    "# Lancement du tracking avec MLFlow\n",
    "with mlflow.start_run(run_name=\"XGBoost Hyperparameter Tuning\"):\n",
    "    \n",
    "    # Grille d'hyperparamètres à tester pour XGBoost\n",
    "    param_grid_xgb = {\n",
    "        'max_depth': [3, 5, 10],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'colsample_bytree': [0.3, 0.7, 1.0],\n",
    "        'subsample': [0.7, 0.8, 1.0],\n",
    "        'gamma': [0, 0.1, 0.3],\n",
    "        'min_child_weight': [1, 5, 10]\n",
    "    }\n",
    "\n",
    "    # Initialisation du modèle XGBoost\n",
    "    model_xgb = xgb.XGBClassifier(use_label_encoder=False, \n",
    "                                  eval_metric='logloss', \n",
    "                                  random_state=42, \n",
    "                                  tree_method='gpu_hist')\n",
    "\n",
    "    # Initialisation de RandomizedSearchCV\n",
    "    random_search_xgb = RandomizedSearchCV(estimator=model_xgb, \n",
    "                                           param_distributions=param_grid_xgb, \n",
    "                                           scoring='roc_auc', \n",
    "                                           cv=5, \n",
    "                                           n_iter=50,  # Nombre d'itérations à tester\n",
    "                                           n_jobs=-1, \n",
    "                                           verbose=3, \n",
    "                                           random_state=42)\n",
    "\n",
    "    # Mesure du temps de recherche des hyperparamètres\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Ajustement sur l'ensemble d'entraînement\n",
    "    random_search_xgb.fit(X_train_split, y_train_split)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calcul du temps écoulé\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    # Afficher les résultats\n",
    "    print(\"Temps pris pour trouver les hyperparamètres :\", elapsed_time, \"secondes\")\n",
    "    print(\"Meilleurs paramètres XGBoost :\", random_search_xgb.best_params_)\n",
    "    print(\"Meilleur score AUC XGBoost :\", random_search_xgb.best_score_)\n",
    "\n",
    "    # Modèle optimisé\n",
    "    best_model_xgb = random_search_xgb.best_estimator_\n",
    "\n",
    "    # Enregistrement des hyperparamètres et des métriques dans MLFlow\n",
    "    mlflow.log_params(random_search_xgb.best_params_)  # Log des meilleurs hyperparamètres\n",
    "    mlflow.log_metric(\"best_auc\", random_search_xgb.best_score_)  # Log du meilleur score AUC\n",
    "    mlflow.log_metric(\"search_time_seconds\", elapsed_time)  # Log du temps pris\n",
    "\n",
    "    # Générer la signature du modèle (entrées/sorties)\n",
    "    signature = infer_signature(X_train_split, best_model_xgb.predict(X_train_split))\n",
    "\n",
    "     # Enregistrement du modèle dans MLFlow et Model Registry avec versioning\n",
    "    mlflow.xgboost.log_model(\n",
    "        best_model_xgb,\n",
    "        artifact_path=\"best_model_xgb\",\n",
    "        signature=signature,\n",
    "        registered_model_name=\"Best_model_xgb-classification\"  # Enregistrement avec nom et versioning\n",
    "    )\n",
    "    \n",
    "    # Enregistrement du modèle optimisé dans MLFlow avec la signature\n",
    "    #mlflow.xgboost.log_model(best_model_xgb, \"best_model_xgb\", signature=signature)\n",
    "\n",
    "    # Terminer le run\n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbdc47a-2d18-49fb-81f9-dc65275d963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grille d'hyperparamètres à tester\n",
    "param_grid_xgb = {\n",
    "    'max_depth': [3, 5, 10],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'colsample_bytree': [0.3, 0.7, 1.0],\n",
    "    'subsample': [0.7, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.3],\n",
    "    'min_child_weight': [1, 5, 10]\n",
    "}\n",
    "\n",
    "model_xgb = xgb.XGBClassifier(use_label_encoder=False, \n",
    "                              eval_metric='logloss', \n",
    "                              random_state=42, \n",
    "                              tree_method='gpu_hist')\n",
    "\n",
    "# Initialisation de RandomizedSearchCV\n",
    "random_search_xgb = RandomizedSearchCV(estimator=model_xgb, \n",
    "                                        param_distributions=param_grid_xgb, \n",
    "                                        scoring={'roc_auc': 'roc_auc', 'accuracy': 'accuracy'},\n",
    "                                        refit='roc_auc', \n",
    "                                        cv=5, \n",
    "                                        n_iter=50,  # Nombre d'itérations à tester\n",
    "                                        n_jobs=-1, \n",
    "                                        verbose=3, \n",
    "                                        random_state=42)\n",
    "\n",
    "\n",
    "# Mesure du temps de recherche des hyperparamètres\n",
    "start_time = time.time()\n",
    "\n",
    "# Ajustement sur l'ensemble d'entraînement\n",
    "random_search_xgb.fit(X_train_split, y_train_split)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Calcul du temps écoulé\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Temps pris pour trouver les hyperparamètres :\", elapsed_time, \"secondes\")\n",
    "\n",
    "# Lancer le tracking global avec MLFlow (pour le meilleur modèle)\n",
    "with mlflow.start_run(run_name=\"XGBoost\"):\n",
    "\n",
    "    # Parcourir tous les résultats de RandomizedSearchCV\n",
    "    for i in range(len(random_search_xgb.cv_results_['params'])):\n",
    "        # Démarrer un run MLflow pour chaque ensemble d'hyperparamètres\n",
    "        with mlflow.start_run(nested=True):  # Run imbriqué sous la run principale\n",
    "\n",
    "            # Récupérer les paramètres et les résultats pour cette itération\n",
    "            params = random_search_xgb.cv_results_['params'][i]\n",
    "            mean_test_auc = random_search_xgb.cv_results_['mean_test_roc_auc'][i]\n",
    "            mean_test_accuracy = random_search_xgb.cv_results_['mean_test_accuracy'][i]\n",
    "            fit_time = random_search_xgb.cv_results_['mean_fit_time'][i]\n",
    "            score_time = random_search_xgb.cv_results_['mean_score_time'][i]\n",
    "\n",
    "            # Enregistrer les hyperparamètres et les métriques dans MLflow\n",
    "            mlflow.log_param(\"Model\", \"XGBoost\")\n",
    "            mlflow.log_params(params)\n",
    "            mlflow.log_metric(\"AUC\", mean_test_auc)\n",
    "            mlflow.log_metric(\"Accuracy\", mean_test_accuracy)\n",
    "            mlflow.log_metric(\"Fit_time_mean\", fit_time)\n",
    "            mlflow.log_metric(\"Score_time_mean\", score_time)\n",
    "\n",
    "    # Enregistrement du meilleur modèle après la recherche\n",
    "    mlflow.log_param(\"Model\", \"XGBoost\")\n",
    "    best_model_xgb = random_search_xgb.best_estimator_\n",
    "    mlflow.log_params(random_search_xgb.best_params_)\n",
    "    mlflow.log_metric(\"AUC\", random_search_xgb.best_score_)\n",
    "    best_index = random_search_xgb.best_index_\n",
    "    best_accuracy = random_search_xgb.cv_results_['mean_test_accuracy'][best_index]\n",
    "    mlflow.log_metric(\"Accuracy\", best_accuracy)\n",
    "    mlflow.log_metric(\"search_time_seconds\", elapsed_time)\n",
    "\n",
    "    # Générer la signature du modèle (entrées/sorties)\n",
    "    signature = infer_signature(X_train_split, best_model_xgb.predict(X_train_split))\n",
    "\n",
    "    # Enregistrer le meilleur modèle dans MLflow\n",
    "    mlflow.xgboost.log_model(\n",
    "        best_model_xgb,\n",
    "        artifact_path=\"XGBoost\",\n",
    "        signature=signature,\n",
    "        registered_model_name=\"XGBoost-classification-model\"\n",
    "    )\n",
    "\n",
    "    print(\"Meilleurs paramètres :\", random_search_xgb.best_params_)\n",
    "    print(\"Meilleur score AUC :\", random_search_xgb.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8a9cc4-8237-4710-902a-316e9a490f44",
   "metadata": {},
   "source": [
    "# 5 - Ajout d'un score métier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee74d5ab-13a3-4346-9ba2-09f97687f54f",
   "metadata": {},
   "source": [
    "Bien que l'accuracy soit élevée, le modèle est surtout influencé par la classe dominante (les clients qui ne font pas défaut). En pratique, cela signifie que le modèle pourrait prédire qu'un client ne fera pas défaut alors qu'il pourrait le faire, entraînant des pertes pour la banque.\n",
    "\n",
    "Pour résoudre ce problème, il est essentiel d'incorporer un score métier avec des pondérations différenciées. Par exemple, une pondération plus forte pourrait être donnée aux faux négatifs (prédire à tort qu'un client est fiable alors qu'il ne l'est pas), car l'impact financier de cette erreur est généralement plus élevé."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ead7d85-d650-4bce-a3f7-a9ea47a8c458",
   "metadata": {},
   "source": [
    "## 5.1 - Création du score métier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea6675f-48f2-4a34-968d-f3ba80b906f1",
   "metadata": {},
   "source": [
    "Pour intégrer un **score métier** dans notre évaluation des modèles, il est nécessaire de formaliser une fonction coût qui pénalise davantage les erreurs de type **Faux Négatifs (FN)**, car le coût pour la banque est plus élevé en cas de défaut de remboursement (prédiction incorrecte d'un bon client alors qu'il ne l'est pas). Nous devons également ajuster le seuil de décision du modèle pour prendre en compte cette fonction coût."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4189cb0f-54df-4bd5-be38-f861d39e1e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def business_cost_function(y_true, y_pred, cost_fn=10, cost_fp=1):\n",
    "    \"\"\"\n",
    "    Calcul du coût métier basé sur les faux positifs et faux négatifs.\n",
    "    \n",
    "    :param y_true: Labels réels\n",
    "    :param y_pred: Prédictions du modèle\n",
    "    :param cost_fn: Coût associé aux Faux Négatifs (par défaut, 10 fois plus que FP)\n",
    "    :param cost_fp: Coût associé aux Faux Positifs (par défaut 1)\n",
    "    :return: Score coût métier total\n",
    "    \"\"\"\n",
    "    # Calcul des Faux Négatifs et Faux Positifs\n",
    "    fn = ((y_true == 1) & (y_pred == 0)).sum()  # Faux Négatifs\n",
    "    fp = ((y_true == 0) & (y_pred == 1)).sum()  # Faux Positifs\n",
    "    \n",
    "    # Coût total en fonction des pondérations\n",
    "    total_cost = (fn * cost_fn) + (fp * cost_fp)\n",
    "    \n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfbcb70-79fd-4acc-88b7-ce4fb382bc3f",
   "metadata": {},
   "source": [
    "Le seuil de décision par défaut est de 0.5 pour classifier une observation en 0 ou 1, mais ce seuil peut ne pas être optimal pour notre cas métier. Nous devons tester différents seuils et trouver celui qui minimise le coût métier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fe296c-2d37-4888-8dc6-6eef1a15e9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimisation du seuil de décision pour minimiser le coût métier\n",
    "def find_optimal_threshold_for_business_score(y_true, y_prob, cost_fn=10, cost_fp=1):\n",
    "    \"\"\"\n",
    "    Trouve le seuil optimal qui minimise le coût métier.\n",
    "    \n",
    "    :param y_true: Labels réels\n",
    "    :param y_prob: Probabilités prédites\n",
    "    :param cost_fn: Coût associé aux Faux Négatifs\n",
    "    :param cost_fp: Coût associé aux Faux Positifs\n",
    "    :return: Seuil optimal\n",
    "    \"\"\"\n",
    "    thresholds = np.arange(0.1, 1.0, 0.01)  # Différents seuils à tester\n",
    "    best_threshold = 0.5\n",
    "    best_cost = float('inf')\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "        cost = business_cost_function(y_true, y_pred, cost_fn, cost_fp)\n",
    "        \n",
    "        if cost < best_cost:\n",
    "            best_cost = cost\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    return best_threshold, best_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3685c8f4-73da-495c-9d11-df01ad09b368",
   "metadata": {},
   "source": [
    "Nous pouvons utiliser cette fonction dans notre fonction d'évaluation pour trouver le meilleur modèle en minimisant le coût métier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cd0831-7d6a-4b55-aa47-50a8cab34fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On adapte notre fonction d'évaluation avec le score métier\n",
    "def evaluate_models_with_business_score(models, X_train_split, X_val, y_train_split, y_val, cost_fn=10, cost_fp=1, log_to_mlflow=False):\n",
    "    # Initialisation d'un DataFrame pour stocker les résultats\n",
    "    results_df = pd.DataFrame(columns=['Model', 'cost_fn', 'cost_fp', 'Best Threshold', 'Business Cost', 'AUC', 'Accuracy', 'Fit_time', 'Score_time'])\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        # Entraînement du modèle avec mesure du temps\n",
    "        start_fit_time = time.time()\n",
    "        model.fit(X_train_split, y_train_split)\n",
    "        fit_time = time.time() - start_fit_time\n",
    "        \n",
    "        # Prédiction des probabilités sur l'ensemble de validation avec mesure du temps\n",
    "        start_score_time = time.time()\n",
    "        y_prob = model.predict_proba(X_val)[:, 1]\n",
    "        score_time = time.time() - start_score_time\n",
    "        \n",
    "        # Trouve le seuil optimal pour minimiser le coût métier\n",
    "        best_threshold, best_cost = find_optimal_threshold_for_business_score(y_val, y_prob, cost_fn, cost_fp)\n",
    "        \n",
    "        # Prédiction avec le meilleur seuil\n",
    "        y_pred = (y_prob >= best_threshold).astype(int)\n",
    "        \n",
    "        # Calcul des métriques classiques\n",
    "        auc_score = roc_auc_score(y_val, y_prob)\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        \n",
    "        # Affichage des résultats\n",
    "        print(f\"Modèle: {name}\")\n",
    "        print(f\"Seuil optimal pour minimiser le coût métier: {best_threshold:.2f}\")\n",
    "        print(f\"Coût métier minimal: {best_cost}\")\n",
    "        print(f\"AUC: {auc_score:.2f}, Accuracy: {accuracy:.2f}\")\n",
    "        print(f\"Fit Time: {fit_time:.2f} seconds, Score Time: {score_time:.2f} seconds\\n\")\n",
    "        \n",
    "        # Affichage de la matrice de confusion et de la courbe AUC-ROC\n",
    "        plot_confusion_matrix_and_roc(y_val, y_prob, y_pred, model_name=name)\n",
    "        \n",
    "        # Enregistrement des résultats dans le DataFrame\n",
    "        new_row = pd.DataFrame({\n",
    "            'Model': [name],\n",
    "            \"cost_fn\": [cost_fn],\n",
    "            \"cost_fp\": [cost_fp],\n",
    "            'Best Threshold': [best_threshold],\n",
    "            'Business Cost': [best_cost],\n",
    "            'AUC': [auc_score],\n",
    "            'Accuracy': [accuracy],\n",
    "            'Fit_time': [fit_time],\n",
    "            'Score_time': [score_time]\n",
    "        })\n",
    "        \n",
    "        results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "        \n",
    "        # Enregistrement dans MLflow si nécessaire\n",
    "        if log_to_mlflow:\n",
    "            with mlflow.start_run(run_name=f\"{name}\"):\n",
    "                mlflow.log_param(\"Model\", name)\n",
    "                mlflow.log_param(\"cost_fn\", cost_fn)\n",
    "                mlflow.log_param(\"cost_fp\", cost_fp)\n",
    "                mlflow.log_param(\"best_threshold\", best_threshold)\n",
    "                mlflow.log_metric(\"business_cost\", best_cost)\n",
    "                mlflow.log_metric(\"AUC\", auc_score)\n",
    "                mlflow.log_metric(\"Accuracy\", accuracy)\n",
    "                mlflow.log_metric(\"Fit_time_mean\", fit_time)\n",
    "                mlflow.log_metric(\"Score_time_mean\", score_time)\n",
    "                # Enregistrement du modèle dans MLFlow et Model Registry avec versioning\n",
    "                mlflow.sklearn.log_model(\n",
    "                    model,\n",
    "                    artifact_path=\"model\",\n",
    "                    signature=signature,\n",
    "                    registered_model_name=f\"{name}-classification-model\"  # Enregistrement avec nom et versioning\n",
    "                )\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132b1124-7b85-4307-9c04-9e029c58e397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix_and_roc(y_val, y_prob, y_pred, model_name=\"Model\"):\n",
    "    # Calcul de la matrice de confusion\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    \n",
    "    # Calcul de la courbe ROC\n",
    "    fpr, tpr, thresholds = roc_curve(y_val, y_prob)\n",
    "    \n",
    "    # Création de la figure et des sous-graphiques\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    # Affichage de la matrice de confusion\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1, cbar=False)\n",
    "    ax1.set_title(f'Confusion Matrix - {model_name}')\n",
    "    ax1.set_xlabel('Predicted label')\n",
    "    ax1.set_ylabel('True label')\n",
    "    \n",
    "    # Affichage de la courbe ROC\n",
    "    ax2.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc_score(y_val, y_prob))\n",
    "    ax2.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    ax2.set_xlim([0.0, 1.0])\n",
    "    ax2.set_ylim([0.0, 1.05])\n",
    "    ax2.set_title(f'ROC Curve - {model_name}')\n",
    "    ax2.set_xlabel('False Positive Rate')\n",
    "    ax2.set_ylabel('True Positive Rate')\n",
    "    ax2.legend(loc=\"lower right\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a83035-d623-446a-8d9f-4c8ad2a329cf",
   "metadata": {},
   "source": [
    "Dans notre cas, étant donné que nous avons déjà effectué une première modélisation et optimisé le seuil de décision pour minimiser le coût métier sur un ensemble de validation, il ne sera pas nécessaire de refaire une cross-validation. Nous avons déjà une estimation raisonnable de la performance des modèles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ece909-e2cc-456e-9edc-76fd41de1626",
   "metadata": {},
   "source": [
    "## 5.2 - Evaluation des modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e2798e-5ed3-49b0-beb9-6694bc195d30",
   "metadata": {},
   "source": [
    "Les paramètres `cost_fn` et `cost_fp` correspondent aux **coûts métier** que nous attribuons aux erreurs de prédiction :\n",
    "\n",
    "- **`cost_fn`** (coût des **faux négatifs**, FN) : C'est le coût associé à une erreur de type **faux négatif**. Un faux négatif se produit lorsque le modèle prédit que le client ne fera **pas défaut** (classe 0), alors qu'en réalité, il fait défaut (classe 1). Dans un contexte de scoring crédit, un faux négatif peut être très coûteux pour la société, car elle pourrait prêter à des clients risqués qui ne rembourseront pas leur prêt.\n",
    "\n",
    "- **`cost_fp`** (coût des **faux positifs**, FP) : C'est le coût associé à une erreur de type **faux positif**. Un faux positif se produit lorsque le modèle prédit que le client fera **défaut** (classe 1), alors qu'en réalité, il ne fait pas défaut (classe 0). Bien que moins risqué qu'un faux négatif, un faux positif peut entraîner un manque à gagner pour la société si elle refuse un prêt à un client qui, en fait, aurait été capable de le rembourser.\n",
    "\n",
    "*Pourquoi ces coûts sont-ils importants ?*\n",
    "\n",
    "Dans la prédiction de défauts de paiement, le coût des erreurs n'est pas symétrique :\n",
    "- Un **faux négatif** (prêter à un client risqué) peut entraîner des pertes financières importantes, d'où un coût métier plus élevé.\n",
    "- Un **faux positif** (refuser un prêt à un bon client) représente un coût moindre en comparaison, car cela conduit à des opportunités manquées mais pas à une perte directe.\n",
    "\n",
    "*Comment sont-ils utilisés ?*\n",
    "\n",
    "Dans la fonction d'évaluation, ces coûts sont utilisés pour trouver le **seuil optimal de classification** qui minimise le coût total pour l'entreprise. Plutôt que de se baser uniquement sur des métriques comme l'accuracy ou l'AUC, nous allons tenir compte des conséquences financières des erreurs de prédiction (FN et FP) pour déterminer le seuil de probabilité optimal. Cela permet de mieux aligner les décisions du modèle avec les intérêts économiques de l'entreprise.\n",
    "\n",
    "En résumé, nous allons ajuster le modèle pour qu'il prenne en compte la **valeur asymétrique des erreurs**, où les faux négatifs sont beaucoup plus coûteux que les faux positifs dans ce contexte de scoring crédit.\n",
    "\n",
    "**On peut dans un premier temps faire l'hypothèse que le coût d’un FN est dix fois supérieur au coût d’un FP**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8bca01b1-572f-4184-a10c-d5422f1a9790",
   "metadata": {},
   "source": [
    "# On va définir le/les modèle(s)\n",
    "models = {\n",
    "    'LGBM Classifier BUSINESS': lgb.LGBMClassifier(random_state=42, device='gpu')\n",
    "}\n",
    "\n",
    "# Appel de la fonction d'évaluation avec enregistrement dans MLflow\n",
    "evaluate_models_with_business_score(models, X_train_split, X_val, y_train_split, y_val, cost_fn=10, cost_fp=1, log_to_mlflow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d46243-4921-46f3-9cf6-8a64f79eccfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On va commencer par initialiser notre expérimentation MLFlow\n",
    "mlflow.set_experiment(\"4 - Score métier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7293b675-fa17-4dbd-ad99-8fb69afec498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On va définir le/les modèle(s) optimisés\n",
    "models = {\n",
    "    #'LGBM Classifier BUSINESS': best_model_lgbm,  # Utilise le modèle LGBM optimisé\n",
    "    'XGBoost': best_model_xgb  # Ajoute également le modèle XGBoost optimisé\n",
    "}\n",
    "\n",
    "# Appel de la fonction d'évaluation avec enregistrement dans MLflow\n",
    "evaluate_models_with_business_score(models, X_train_split, X_val, y_train_split, y_val, cost_fn=10, cost_fp=1, log_to_mlflow=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9443af51-1ab2-41ab-9c53-c4ad570890a6",
   "metadata": {},
   "source": [
    "## 5.3 - Essais empiriques pour ajuster cost_fn et cost_fp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4376ff-7ceb-4a11-8b95-dade05f51ba7",
   "metadata": {},
   "source": [
    "Trouver les valeurs optimales de **`cost_fn`** (coût des faux négatifs) et **`cost_fp`** (coût des faux positifs) fait partie de l'ajustement du modèle en fonction des objectifs métier, mais ce n'est pas techniquement un réglage classique des hyperparamètres du modèle lui-même.\n",
    "\n",
    "*Comment déterminer `cost_fn` et `cost_fp` ?*\n",
    "\n",
    "Nous pouvons ajuster les valeurs de `cost_fn` et `cost_fp` en fonction de l'impact métier que nous souhaitons minimiser. Voici quelques approches pour les déterminer :\n",
    "\n",
    "1. **Basé sur les données métier** : \n",
    "   - Nous pourrions discuter avec les experts métier pour obtenir une estimation du **coût réel** d'un faux négatif (perte sur un prêt défaillant) et d'un faux positif (perte d'un client solvable).\n",
    "   - Exemple : Si la perte moyenne due à un défaut de paiement est de 10 000 euros et la marge bénéficiaire sur un client solvable est de 1 000 euros, alors `cost_fn` pourrait être fixé à 10 (ou 10 000) et `cost_fp` à 1 (ou 1 000).\n",
    "\n",
    "2. **Essais empiriques** (essais/erreurs) :\n",
    "   - Nous pouvons effectivement tester différentes combinaisons de `cost_fn` et `cost_fp` pour voir lesquelles minimisent le coût total de prédiction.\n",
    "   - Par exemple, nous pouvons essayer plusieurs valeurs (ex: `cost_fn=5, 10, 15` et `cost_fp=1, 2, 5`) et voir lesquelles donnent les résultats les plus satisfaisants en termes de coût métier global.\n",
    "\n",
    "Ensuite, nous pouvons analyser les résultats pour déterminer quelle combinaison minimise le coût global tout en maintenant des performances satisfaisantes (AUC, accuracy, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfdfd6c-2d84-4542-a21d-21fb0243c03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation d'un DataFrame pour stocker tous les résultats\n",
    "all_results_df = pd.DataFrame(columns=['Model', 'Best Threshold', 'Business Cost', 'AUC', 'Accuracy', 'Fit_time', 'Score_time'])\n",
    "\n",
    "# Essais pour différentes valeurs de cost_fn et cost_fp\n",
    "cost_fn_values = [5, 10, 15]\n",
    "cost_fp_values = [1, 2, 5]\n",
    "\n",
    "for cost_fn in cost_fn_values:\n",
    "    for cost_fp in cost_fp_values:\n",
    "        print(f\"Test avec cost_fn={cost_fn} et cost_fp={cost_fp}\")\n",
    "        results_df = evaluate_models_with_business_score(models, X_train_split, X_val, y_train_split, y_val, cost_fn=cost_fn, cost_fp=cost_fp, log_to_mlflow=True)\n",
    "        all_results_df = pd.concat([all_results_df, results_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910af98c-8964-4a3b-bbd5-aa1dcac9a353",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nRésumé complet des résultats :\")\n",
    "all_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4190e469-f2c2-4b2a-b92c-e475f5334c9a",
   "metadata": {},
   "source": [
    "1. **Performance Globale :**\n",
    "   - **AUC Constante :** Le score AUC est constant pour toutes les combinaisons de `cost_fn` et `cost_fp`. Cela signifie que le modèle a une capacité de discrimination stable, quelle que soit la combinaison de coûts.\n",
    "   - **Accuracy Variable :** L'accuracy varie entre **0.77** et **0.92**, montrant que le choix du seuil affecte la performance.\n",
    "\n",
    "2. **Seuil Optimal :** \n",
    "   - Le seuil optimal varie en fonction des valeurs de `cost_fn` et `cost_fp`. Par exemple, avec `cost_fn=5` et `cost_fp=1`, le meilleur seuil est **0.17**, tandis qu'avec `cost_fn=5` et `cost_fp=5`, le seuil optimal est **0.50**.\n",
    "   - Ces seuils reflètent la façon dont le modèle priorise les prédictions en fonction des coûts d'erreur associés.\n",
    "\n",
    "3. **Coût Métier :**\n",
    "   - Le coût métier varie en fonction des valeurs de `cost_fn` et `cost_fp`. Par exemple, le coût minimal est de **30339** pour `cost_fn=5` et `cost_fp=1`, tandis qu'il monte à **102245** avec `cost_fn=15` et `cost_fp=5`.\n",
    "   - Cela montre que les choix des coûts ont un impact significatif sur les résultats économiques de votre modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1730cfd-cd95-4b9c-a6d9-136f57383604",
   "metadata": {},
   "source": [
    "- Trouver les **coûts métier** optimaux (`cost_fn` et `cost_fp`) est un processus empirique qui **dépend des objectifs spécifiques** de l'entreprise et des conséquences financières des erreurs de classification.\n",
    "- Nous avons essayer ici via des essais empiriques.\n",
    "- Ce n'est pas un réglage des **hyperparamètres du modèle**, mais c'est crucial pour aligner les prédictions du modèle avec les **objectifs métier**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40960fc2-31fa-4356-b261-bd76df21a471",
   "metadata": {},
   "source": [
    "# 6 - Modèle final optimisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfe387f-963d-42b8-9afb-0c09be7fff49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_final_model(best_model, X_val, y_val, best_threshold, cost_fn=cost_fn, cost_fp=cost_fp, log_to_mlflow=False):\n",
    "    # Prédiction des probabilités sur l'ensemble de validation\n",
    "    start_score_time = time.time()\n",
    "    y_prob = best_model.predict_proba(X_val)[:, 1]\n",
    "    score_time = time.time() - start_score_time\n",
    "    \n",
    "    # Prédiction avec le meilleur seuil\n",
    "    y_pred = (y_prob >= best_threshold).astype(int)\n",
    "    \n",
    "    # Calcul des métriques\n",
    "    auc_score = roc_auc_score(y_val, y_prob)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "\n",
    "    # Calcul des Faux Négatifs et Faux Positifs\n",
    "    fn = ((y_val == 1) & (y_pred == 0)).sum()  # Faux Négatifs\n",
    "    fp = ((y_val == 0) & (y_pred == 1)).sum()  # Faux Positifs\n",
    "    \n",
    "    # Coût total en fonction des pondérations\n",
    "    total_cost = (fn * cost_fn) + (fp * cost_fp)\n",
    "\n",
    "    # Matrice de confusion\n",
    "    conf_matrix = confusion_matrix(y_val, y_pred)\n",
    "    conf_matrix_df = pd.DataFrame(conf_matrix, \n",
    "                                   index=['Vraie classe 0', 'Vraie classe 1'], \n",
    "                                   columns=['Prédite classe 0', 'Prédite classe 1'])\n",
    "\n",
    "    # Affichage des résultats\n",
    "    print(f\"Modèle: Best Model\")\n",
    "    print(f\"Seuil optimal pour minimiser le coût métier: {best_threshold:.2f}\")\n",
    "    print(f\"Coût métier calculé: {total_cost}\")\n",
    "    print(f\"AUC: {auc_score:.2f}, Accuracy: {accuracy:.2f}\")\n",
    "    print(\"\\nMatrice de Confusion :\\n\", conf_matrix_df)\n",
    "\n",
    "    # Affichage de la matrice de confusion et de la courbe AUC-ROC\n",
    "    plot_confusion_matrix_and_roc(y_val, y_prob, y_pred, model_name=\"Best Model\")\n",
    "    \n",
    "    # Enregistrement dans MLflow si nécessaire\n",
    "    if log_to_mlflow:\n",
    "        with mlflow.start_run(run_name=\"Best Model Evaluation\"):\n",
    "            mlflow.log_param(\"Model\", model_name)\n",
    "            mlflow.log_param(\"cost_fn\", cost_fn)\n",
    "            mlflow.log_param(\"cost_fp\", cost_fp)\n",
    "            mlflow.log_param(\"best_threshold\", best_threshold)\n",
    "            mlflow.log_metric(\"business_cost\", total_cost)\n",
    "            mlflow.log_metric(\"AUC\", auc_score)\n",
    "            mlflow.log_metric(\"Accuracy\", accuracy)\n",
    "            mlflow.log_metric(\"Score_time_mean\", score_time)\n",
    "            # Enregistrement du modèle dans MLFlow et Model Registry avec versioning\n",
    "            mlflow.xgboost.log_model(\n",
    "                best_model,\n",
    "                artifact_path=\"model\",\n",
    "                signature=signature,\n",
    "                registered_model_name=f\"{model_name}-classification-model\"  # Enregistrement avec nom et versioning\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe74f836-c04e-415e-b7bb-80d14491fdfc",
   "metadata": {},
   "source": [
    "## 6.1 - Analyse des Différents Seuils et Coûts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fa779e-b141-406a-a66b-d91cc9928ae4",
   "metadata": {},
   "source": [
    "Nous allons créer une échelle de référence pour le coût métier qui aide les utilisateurs à interpréter les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6023b0-9573-4879-b5c6-53ce6fd06a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On va ajouter une colonne d'aide à l'interprétation pour le business_cost\n",
    "q1 = all_results_df['Business Cost'].quantile(0.25)\n",
    "q3 = all_results_df['Business Cost'].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "\n",
    "def categorize_cost(cost):\n",
    "    if cost < q1:\n",
    "        return \"Faible Coût\"\n",
    "    elif q1 <= cost < q3:\n",
    "        return \"Coût Modéré\"\n",
    "    elif q3 <= cost < (q3 + 1.5 * iqr):\n",
    "        return \"Coût Élevé\"\n",
    "    else:\n",
    "        return \"Coût Très Élevé\"\n",
    "\n",
    "all_results_df['Cost Category'] = all_results_df['Business Cost'].apply(categorize_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c7b210-ae19-48a2-9e42-47cc816ff5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nRésumé complet des résultats sur l'entraînement avec business_cost :\")\n",
    "all_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cff68f8-928f-48cc-b565-97ea324c7714",
   "metadata": {},
   "source": [
    "Nous avons utilisé les statistiques calculées pour établir des catégories :\n",
    "\n",
    "- **Faible Coût** : Coût inférieur au premier quartile.\n",
    "- **Coût Modéré** : Coût entre le premier et le troisième quartile.\n",
    "- **Coût Élevé** : Coût au-dessus du troisième quartile mais inférieur à 1,5 fois l'écart interquartile.\n",
    "- **Coût Très Élevé** : Coût au-dessus de 1,5 fois l'écart interquartile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aff2b7f-56c9-4c23-acec-9cb67f730d13",
   "metadata": {},
   "source": [
    "Les **catégories de coût** sont classées en trois niveaux : **Faible Coût**, **Coût Modéré**, et **Coût Élevé**. Voici une analyse de chaque catégorie :\n",
    "\n",
    "- **Faible Coût (≤ 35 000)** : Les seuils autour de 0.17 et 0.30 avec des pondérations de coût faibles pour les faux négatifs (cost_fn = 5) et les faux positifs (cost_fp = 1 ou 2) permettent d'obtenir un faible coût métier tout en maintenant un bon équilibre entre sensibilité et précision.\n",
    "**Meilleur compromis** : Le seuil de **0.17** avec un coût de **30 339** semble être une bonne option avec une **AUC de 0.774** et une **accuracy de 0.865**. Cela montre que le modèle est performant avec un coût relativement bas.\n",
    "  \n",
    "- **Coût Modéré (entre 35 000 et 65 000)** : Dans cette gamme, des pondérations plus élevées pour les faux négatifs (cost_fn = 10 ou 15) augmentent le coût métier. Par exemple, à **0.10** avec cost_fn = 10, le coût passe à **46 518**. Ici, la précision diminue légèrement, mais l'AUC reste la même.\n",
    "Cette plage de coûts peut être acceptable si le risque de prédire un défaut est plus tolérable pour l'entreprise.\n",
    "\n",
    "- **Coût Élevé (> 65 000)** : Dans ces configurations, les pondérations pour les faux négatifs et/ou les faux positifs sont élevées, ce qui entraîne une augmentation significative du coût métier.\n",
    "**Exemple** : À **0.28** avec cost_fn = 15 et cost_fp = 5, le coût métier atteint **102 245**. Ce type de configuration est probablement trop coûteux, à moins que l'entreprise ne veuille absolument minimiser les faux négatifs, même au détriment des faux positifs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5671c63-11cb-41b2-9e34-f7464353141b",
   "metadata": {},
   "source": [
    "## 6.2 - Meilleures configurations en fonction du contexte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f2b767-4ae3-4418-8556-7cda4636189e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On va commencer par initialiser notre expérimentation MLFlow\n",
    "mlflow.set_experiment(\"5 - Modèle final optimisé\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a0dd74-3dd7-4f84-9892-e8e2e34ea23f",
   "metadata": {},
   "source": [
    "#### 6.2.1 - Objectif : Minimiser le Coût tout en Conservant une Bonne Précision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478ab18a-1a39-46a9-a6e8-59c2bc8b8d14",
   "metadata": {},
   "source": [
    "- Le **seuil de 0.17** avec un coût métier de **30 339** et une **accuracy de 0.865** semble optimal. Il permet de maintenir un bon équilibre entre les deux classes tout en maintenant un coût faible. Ce modèle est particulièrement adapté si l'entreprise peut tolérer un certain nombre de faux positifs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497b84bd-dd70-47ed-a14c-e582e5c0978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les valeurs de notre coût métier\n",
    "threshold = 0.17  # Remplacer par la valeur souhaitée\n",
    "cost_fn = 5\n",
    "cost_fp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb90100-2e86-4c05-8483-ee8b40332367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appel de la fonction d'évaluation avec les bons arguments\n",
    "evaluate_final_model(best_model_xgb, X_val, y_val, threshold, cost_fn, cost_fp, log_to_mlflow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b95fd0-4d7f-4c1a-9a4f-decc1cc12455",
   "metadata": {},
   "source": [
    "- **Faux Négatifs (FN)** : 4 429 clients en défaut ont été classés comme \"Non-défaut\", ce qui est le coût le plus important pour l'entreprise. Le coût métier de **30 295** est directement influencé par ce chiffre, compte tenu du paramétrage que tu as fait pour `cost_fn = 5` et `cost_fp = 1`.\n",
    "  \n",
    "- **Faux Positifs (FP)** : 8 150 clients qui n’étaient pas en défaut ont été prédits à tort comme étant en défaut. Ce chiffre est élevé, mais comme le coût des faux positifs (`cost_fp`) est moindre (1), leur impact sur le coût total est faible.\n",
    "\n",
    "\n",
    "Le coût métier est calculé ainsi :\n",
    "- Coût métier = (FN * `cost_fn`) + (FP * `cost_fp`)\n",
    "- **Coût métier = (4 429 * 5) + (8 150 * 1) = 22 145 + 8 150 = 30 295**\n",
    "\n",
    "Ce coût représente bien un compromis entre la minimisation des faux négatifs et la gestion des faux positifs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f967685-6a9e-48d6-bc6d-939504060a20",
   "metadata": {},
   "source": [
    "#### 6.2.2 - Objectif : Tolérer un Coût Modéré pour Améliorer la Précision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a62235-ba74-4ac1-9881-f0195e4ece1c",
   "metadata": {},
   "source": [
    "- Si l'objectif est d'améliorer légèrement la précision globale tout en gardant un coût modéré, un **seuil de 0.50** avec un coût de **36 810** (cost_fn = 5, cost_fp = 5) pourrait être un bon compromis. Cela augmente la précision à **0.920**, ce qui peut être plus favorable dans un contexte où une légère hausse de coût est acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a223bf9-8b9e-44e1-a01a-17f5396af895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les valeurs de notre coût métier\n",
    "threshold = 0.5  # Remplacer par la valeur souhaitée\n",
    "cost_fn = 5\n",
    "cost_fp = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e673c4-39a5-402c-b84c-fc981412ec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appel de la fonction d'évaluation avec les bons arguments\n",
    "evaluate_final_model(best_model_xgb, X_val, y_val, threshold, cost_fn, cost_fp, log_to_mlflow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0006cfb3-6f41-46f1-997f-c22041f59aa6",
   "metadata": {},
   "source": [
    "Dans ce scénario, nous avons un coût égal pour les faux négatifs et les faux positifs (`cost_fn = 5` et `cost_fp = 5`), ce qui signifie que les deux types d'erreurs sont considérés comme ayant le même impact sur le business.\n",
    "\n",
    "Le coût métier est calculé ainsi :\n",
    "- Coût métier = (FN * `cost_fn`) + (FP * `cost_fp`)\n",
    "- **Coût métier = (7 102 * 5) + (243 * 5) = 35 510 + 1 215 = 36 725**\n",
    "\n",
    "Ce coût reflète la tolérance à un **niveau modéré de faux négatifs et de faux positifs**, car les deux erreurs ont été pondérées de manière égale.\n",
    "\n",
    "**Analyse du Seuil et des Erreurs**\n",
    "- En augmentant le **seuil à 0.50**, le modèle devient beaucoup plus conservateur, ne prédisant que très rarement la classe \"défaut\". Cela est visible dans la matrice de confusion, où il y a **seulement 311 vraies prédictions de défauts** et **243 faux positifs**.\n",
    "  \n",
    "- Le **nombre élevé de faux négatifs (7 102)** indique que de nombreux clients qui feront défaut ne sont pas correctement identifiés, ce qui peut entraîner des pertes financières importantes pour l'entreprise.\n",
    "\n",
    "- **Faux positifs (243)** : Le faible nombre de faux positifs indique que très peu de clients sans défaut sont incorrectement classés comme \"défaut\", ce qui améliore la précision globale du modèle et réduit les conséquences pour les clients non défaillants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a24ad1-41bf-4f03-96e0-b9a80d9b2548",
   "metadata": {},
   "source": [
    "**Avantages de cette configuration** : Très haute précision et très faible nombre de faux positifs, ce qui minimise les perturbations pour les clients sans défaut. Cela est avantageux pour des cas où il est essentiel de ne pas rejeter à tort les clients sans problème.\n",
    "  \n",
    "**Inconvénients** : Le coût métier est plus élevé que dans d'autres configurations car beaucoup de clients en défaut ne sont pas correctement identifiés. Si les faux négatifs représentent un risque élevé pour l'entreprise (comme dans le cas des défauts de paiement), ce compromis pourrait être risqué."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b017235-ec5d-44a5-9d28-e6efe1b35507",
   "metadata": {},
   "source": [
    "#### 6.2.3 - Objectif : Réduire le Risque des Faux Négatifs à Tout Prix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f40132a-1d8f-4882-a05b-06ad2ad6ef16",
   "metadata": {},
   "source": [
    "- Si l'accent est mis sur la réduction des faux négatifs pour éviter des pertes majeures, des seuils avec un coût élevé comme **0.28** (cost_fn = 15, cost_fp = 5) peuvent être envisagés, bien que le coût métier soit très élevé (**102 245**). Cette configuration pourrait être justifiée si les pertes liées aux défauts de paiement sont très coûteuses pour l'entreprise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903a012a-0372-49b8-9639-b8919e54c939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les valeurs de notre coût métier\n",
    "threshold = 0.28  # Remplacer par la valeur souhaitée\n",
    "cost_fn = 15\n",
    "cost_fp = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455466fb-8506-479b-b1a2-b389a05d6306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appel de la fonction d'évaluation avec les bons arguments\n",
    "evaluate_final_model(best_model_xgb, X_val, y_val, threshold, cost_fn, cost_fp, log_to_mlflow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a01f4c7-18cc-4f3d-9938-9e8bd503f56a",
   "metadata": {},
   "source": [
    "Dans cette configuration, nous avons fortement **pondéré le coût des faux négatifs** (`cost_fn = 15`) par rapport aux faux positifs (`cost_fp = 5`), montrant que les erreurs de type faux négatif sont beaucoup plus coûteuses pour l'entreprise.\n",
    "\n",
    "Le coût métier est calculé ainsi :\n",
    "- Coût métier = (FN * `cost_fn`) + (FP * `cost_fp`)\n",
    "- **Coût métier = (5 884 * 15) + (2 662 * 5) = 88 260 + 13 310 = 101 570**\n",
    "\n",
    "Le coût métier est très élevé, car nous tolèrons peu de faux négatifs, ce qui est en ligne avec l'objectif de réduire le risque de ces erreurs à tout prix.\n",
    "\n",
    "**Analyse du Seuil et des Erreurs**\n",
    "\n",
    "- Avec un **seuil de 0.28**, le modèle devient plus sensible aux défauts, prédisant plus souvent la classe \"défaut\" que dans des configurations plus conservatrices. Cela est visible dans le nombre de **prédictions positives (défaut)**, où 1 529 clients en défaut sont correctement identifiés.\n",
    "  \n",
    "- **Faux négatifs** : 5 884 clients en défaut ne sont pas détectés, ce qui est encore un risque important, mais inférieur à des configurations plus conservatrices comme avec un seuil de 0.50 (7 102 faux négatifs).\n",
    "\n",
    "- **Faux positifs** : 2 662 clients sans défaut sont classés comme \"défaut\", ce qui reste relativement faible et acceptable dans ce contexte où l'accent est mis sur la réduction des faux négatifs.\n",
    "\n",
    "**Avantages de cette configuration** : Ce scénario minimise le risque lié aux faux négatifs, ce qui est crucial si les défauts de paiement non détectés ont un impact majeur sur l'entreprise. Bien que le coût métier soit très élevé, tu réduis efficacement le nombre de clients défaillants non détectés.\n",
    "\n",
    "**Inconvénients** : Le coût métier est extrêmement élevé (101 570), et bien que le nombre de faux positifs soit relativement faible, tu sacrifies une partie de la précision globale pour éviter les défauts non détectés."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b04bfc4-73b8-4d3f-84e6-d7fbea0e357d",
   "metadata": {},
   "source": [
    "## 6.3 - Analyse de ces différentes configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01256645-83f8-4f31-8c5c-04bba9e64cdf",
   "metadata": {},
   "source": [
    "L'objectif principal de cette analyse était de trouver un équilibre optimal entre la **réduction des erreurs de classification** (faux positifs et faux négatifs) et la **minimisation du coût métier**, en tenant compte des risques financiers pour l'entreprise.\n",
    "\n",
    "**Résumé des Scénarios Testés :**\n",
    "\n",
    "- **A. Minimiser le Coût tout en Conservant une Bonne Précision (Seuil = 0.17)** :\n",
    "  - **Coût métier** : 30 295\n",
    "  - **Précision (Accuracy)** : 86%\n",
    "  - **AUC** : 0.78\n",
    "  - Cette configuration offre un compromis équilibré entre le coût métier et la précision, avec un bon contrôle sur les faux négatifs (4 429) et faux positifs (8 150). C’est une solution idéale si l’entreprise cherche à maintenir un bon équilibre entre précision et coût sans privilégier l'un ou l'autre de manière extrême.\n",
    "\n",
    "- **B. Tolérer un Coût Modéré pour Améliorer la Précision (Seuil = 0.50)** :\n",
    "  - **Coût métier** : 36 725\n",
    "  - **Précision (Accuracy)** : 92%\n",
    "  - **AUC** : 0.78\n",
    "  - Cette configuration maximise la précision, réduisant le nombre de faux positifs (243) de manière significative tout en augmentant les faux négatifs (7 102). Elle est adaptée si l'entreprise veut un modèle hautement précis, mais au prix d'une hausse des clients en défaut non détectés.\n",
    "\n",
    "- **C. Réduire le Risque des Faux Négatifs à Tout Prix (Seuil = 0.28)** :\n",
    "  - **Coût métier** : 101 570\n",
    "  - **Précision (Accuracy)** : 91%\n",
    "  - **AUC** : 0.78\n",
    "  - Ici, l'accent est mis sur la réduction des faux négatifs (5 884), avec une pondération élevée de cette erreur dans le calcul du coût métier. Le coût métier est élevé, mais ce scénario minimise le risque de non-détection des défauts de paiement, crucial pour éviter des pertes financières importantes.\n",
    "\n",
    "**Interprétation des Résultats :**\n",
    "\n",
    "- **Précision et AUC Stables** : Tous les scénarios montrent une **AUC stable de 0.78**, indiquant une bonne capacité du modèle à différencier les classes \"défaut\" et \"non-défaut\". Toutefois, la précision (accuracy) varie en fonction du seuil choisi, reflétant les compromis entre la détection des défauts et le contrôle des faux positifs.\n",
    "  \n",
    "- **Coût Métier** : Le coût métier augmente considérablement lorsque l’on privilégie la réduction des faux négatifs (scénario C), soulignant qu'une approche plus stricte pour identifier les défauts non détectés peut être coûteuse, mais nécessaire dans certains contextes.\n",
    "\n",
    "**Recommandations :**\n",
    "\n",
    "- **Scénario A (Seuil = 0.17)** est une option idéale si l'entreprise souhaite un bon équilibre entre le contrôle des erreurs et la gestion des coûts. Ce seuil maintient un coût métier raisonnable tout en offrant une bonne précision et une répartition équilibrée des faux positifs et faux négatifs.\n",
    "  \n",
    "- **Scénario B (Seuil = 0.50)** convient mieux si l’objectif est d'augmenter la précision globale, à condition que l’entreprise soit prête à accepter plus de clients en défaut non détectés (faux négatifs). Cela pourrait convenir dans des cas où une légère augmentation des défauts est acceptable pour améliorer la précision générale.\n",
    "  \n",
    "- **Scénario C (Seuil = 0.28)** est recommandé si l’objectif principal est de minimiser les risques associés aux faux négatifs, notamment dans des contextes où les défauts non détectés sont extrêmement coûteux pour l'entreprise. Cependant, cela vient au prix d’un coût métier élevé.\n",
    "\n",
    "**Conclusion :**\n",
    "\n",
    "La **sélection du seuil optimal** dépend du contexte métier et des priorités de l’entreprise en matière de gestion du risque. Si l'objectif est de **minimiser les pertes financières dues aux défauts de paiement non détectés**, alors un seuil plus bas (ex. 0.28) avec une tolérance pour un coût métier élevé pourrait être justifié. Cependant, si l'objectif est de **maximiser la précision globale du modèle**, un seuil plus élevé (ex. 0.50) serait préférable. **Le scénario avec un seuil à 0.17** offre un compromis intéressant et semble être le plus polyvalent, avec un bon équilibre entre précision et coût.\n",
    "\n",
    "Le choix final doit être aligné avec la **stratégie de risque** et les **objectifs financiers** de l'entreprise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaa57b0-dc04-4eb7-b8f5-1d5eabb7c611",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f699ca3d-2192-41e2-8e4e-7eeb15b589dd",
   "metadata": {},
   "source": [
    "# 7 - Features importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d236cc-b273-4e1e-a169-b8748b74ee9a",
   "metadata": {},
   "source": [
    "## 7.1 - Préparation des données de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8ec251-983e-423e-a2c5-869aa9423f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spécifier le chemin du modèle\n",
    "model_path = \"file:///C:/Users/mauge/Documents/github/P7_implementer_modele_scoring/mlartifacts/535513794895831126/144f60891d2140538a6daad907da28a3/artifacts/model\"\n",
    "data_path = \"C:/Users/mauge/Documents/github/P7_implementer_modele_scoring/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dcd963-75d6-4201-bac6-0e759a47c48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle à partir du chemin local\n",
    "model = mlflow.xgboost.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ffc523-6f97-499b-b39b-487ad027822a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clients = pipeline_features_eng.execute_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a81df3-695b-4890-a94c-4d99892004e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clients.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae87bbc1-daeb-4bb5-bef7-a617a80369ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supposons que vous ayez un DataFrame d'entraînement appelé X_train\n",
    "# et un DataFrame des clients à vérifier appelé df_clients\n",
    "\n",
    "# Affichez le nombre de colonnes dans les deux DataFrames\n",
    "print(f\"Nombre de colonnes dans X_train : {X_train.shape[1]}\")\n",
    "print(f\"Nombre de colonnes dans df_clients : {df_clients.shape[1]}\")\n",
    "\n",
    "# Vérifiez les colonnes manquantes\n",
    "missing_columns = set(X_train.columns) - set(df_clients.columns)\n",
    "if missing_columns:\n",
    "    print(f\"Colonnes manquantes dans df_clients : {missing_columns}\")\n",
    "else:\n",
    "    print(\"Aucune colonne manquante dans df_clients.\")\n",
    "\n",
    "# Vérifiez les colonnes supplémentaires dans df_clients\n",
    "extra_columns = set(df_clients.columns) - set(X_train.columns)\n",
    "if extra_columns:\n",
    "    print(f\"Colonnes supplémentaires dans df_clients : {extra_columns}\")\n",
    "else:\n",
    "    print(\"Aucune colonne supplémentaire dans df_clients.\")\n",
    "\n",
    "# Vérifiez si les colonnes sont dans le bon ordre\n",
    "columns_are_equal = (list(X_train.columns) == list(df_clients.columns))\n",
    "if columns_are_equal:\n",
    "    print(\"Les colonnes de df_clients sont dans le même ordre que celles de X_train.\")\n",
    "else:\n",
    "    print(\"Les colonnes de df_clients ne sont pas dans le même ordre que celles de X_train.\")\n",
    "\n",
    "# Afficher les colonnes dans le mauvais ordre\n",
    "if not columns_are_equal:\n",
    "    print(\"Ordre des colonnes dans df_clients :\")\n",
    "    print(df_clients.columns.tolist())\n",
    "    print(\"Ordre des colonnes dans X_train :\")\n",
    "    print(X_train.columns.tolist())\n",
    "\n",
    "# Assurez-vous de conserver SK_ID_CURR et TARGET dans df_clients\n",
    "# Créer une liste des colonnes à inclure\n",
    "columns_to_include = ['SK_ID_CURR', 'TARGET'] + [col for col in X_train.columns if col not in ['SK_ID_CURR', 'TARGET']]\n",
    "\n",
    "# Réajuster les colonnes de df_clients pour qu'elles soient dans le même ordre que celles de X_train\n",
    "# On vérifie d'abord que les colonnes à inclure existent bien dans df_clients\n",
    "columns_to_include = [col for col in columns_to_include if col in df_clients.columns]\n",
    "df_clients = df_clients[columns_to_include]\n",
    "\n",
    "# Vérifiez les valeurs manquantes\n",
    "if df_clients.isnull().any().any():\n",
    "    print(\"df_clients contient des valeurs manquantes.\")\n",
    "else:\n",
    "    print(\"Aucune valeur manquante dans df_clients.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c64b5a3-2604-49a6-8112-291094bcf60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supposons que vous avez déjà vos DataFrames X_train et df_clients\n",
    "\n",
    "# Créer une liste des colonnes à inclure, y compris SK_ID_CURR et TARGET\n",
    "columns_to_include = ['SK_ID_CURR', 'TARGET'] + [col for col in X_train.columns if col not in ['SK_ID_CURR', 'TARGET']]\n",
    "\n",
    "# Réajuster les colonnes de df_clients pour qu'elles soient dans le même ordre que celles de X_train, tout en conservant SK_ID_CURR et TARGET\n",
    "df_clients = df_clients[columns_to_include]\n",
    "\n",
    "# Vérifiez que l'ordre a bien été ajusté\n",
    "print(\"Ordre des colonnes dans df_clients après réajustement :\")\n",
    "print(df_clients.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7527244f-6a15-40c2-b6e2-89ba002ef7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supposons que vous avez vos DataFrames X_train et df_clients\n",
    "\n",
    "# Vérifiez le nombre de colonnes\n",
    "num_columns_X_train = X_train.shape[1]\n",
    "num_columns_df_clients = df_clients.shape[1]\n",
    "\n",
    "if num_columns_X_train == num_columns_df_clients:\n",
    "    print(\"Les deux DataFrames ont le même nombre de colonnes.\")\n",
    "else:\n",
    "    print(f\"Nombre de colonnes dans X_train : {num_columns_X_train}\")\n",
    "    print(f\"Nombre de colonnes dans df_clients : {num_columns_df_clients}\")\n",
    "    print(\"Les deux DataFrames n'ont pas le même nombre de colonnes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c65b5db-e74e-4bb6-aeee-8e4f2ddf9683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrer les clients selon SK_ID_CURR\n",
    "clients_data = df_clients[df_clients['SK_ID_CURR'].isin(df_application_test['SK_ID_CURR'])]\n",
    "\n",
    "# Créer une instance de MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Colonnes à exclure de la mise à l'échelle\n",
    "exclude_cols = ['SK_ID_CURR', 'TARGET']\n",
    "\n",
    "# Sélectionner les colonnes numériques à scaler tout en excluant les colonnes spécifiées\n",
    "columns_to_scale = clients_data.select_dtypes(include=['float64', 'int64']).columns.difference(exclude_cols)\n",
    "\n",
    "# Appliquer le scaler uniquement sur les colonnes sélectionnées\n",
    "clients_data_scaled = clients_data.copy()  # Crée une copie pour garder df_clients propre\n",
    "clients_data_scaled[columns_to_scale] = scaler.fit_transform(clients_data[columns_to_scale])\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"Clients Data d'origine :\")\n",
    "print(clients_data.head())\n",
    "print(\"\\nClients Data mises à l'échelle :\")\n",
    "print(clients_data_scaled.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04552cab-5d14-4f7c-a906-8cbdd173d8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(df_application_test.shape)\n",
    "print(df_clients.shape)\n",
    "print(clients_data.shape)\n",
    "print(clients_data_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d083142-f0ee-4454-b763-bbe60161bc5c",
   "metadata": {},
   "source": [
    "## 7.2 - Features importance locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b1e581-f419-4a3f-bcb8-c29c5780e284",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = [f for f in clients_data.columns if f not in ['TARGET','SK_ID_CURR']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ac6453-abe2-4056-b306-090ff9be36a2",
   "metadata": {},
   "source": [
    "### 7.2.1 - Avec thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f0ecb7-4540-46f3-8246-31e7d6273d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des seuils avec leurs noms associés du business score\n",
    "thresholds = {\n",
    "    \"sans\": {\"valeur\": 0.05, \"nom\": \"Sans risque\"},\n",
    "    \"faible\": {\"valeur\": 0.17, \"nom\": \"Faible coût\"},\n",
    "    \"modere\": {\"valeur\": 0.50, \"nom\": \"Coût modéré\"},\n",
    "    \"eleve\": {\"valeur\": 0.28, \"nom\": \"Coût élevé\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f383e7e0-8b38-47b7-9725-8bbc7344f25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(client_id, seuil_nom=\"faible\"):\n",
    "    \"\"\"\n",
    "    Effectue la prédiction du score de crédit pour un client donné en utilisant le seuil spécifié.\n",
    "    Retourne la probabilité, la classe prédite (0 ou 1), les features utilisées et le seuil.\n",
    "    \"\"\"\n",
    "    # Récupérer le client sélectionné dans le DataFrame\n",
    "    selected_client = clients_data_scaled.loc[clients_data_scaled['SK_ID_CURR'] == client_id]\n",
    "    \n",
    "    # Assurez-vous que 'selected_client' contient bien un client\n",
    "    if selected_client.empty:\n",
    "        raise ValueError(f\"Aucun client trouvé avec SK_ID_CURR = {client_id}\")\n",
    "    \n",
    "    # Calcul de la probabilité de la classe positive\n",
    "    proba = model.predict_proba(selected_client[feats])[0][1]  # Probabilité de défaut\n",
    "    \n",
    "    # Récupérer le seuil correspondant et son nom\n",
    "    seuil_info = thresholds[seuil_nom]\n",
    "    seuil_valeur = seuil_info[\"valeur\"]\n",
    "    seuil_nom_affiche = seuil_info[\"nom\"]\n",
    "    \n",
    "    # Faire la prédiction en fonction du seuil\n",
    "    prediction = int(proba >= seuil_valeur)  # Prédiction basée sur le seuil choisi (0 ou 1)\n",
    "    \n",
    "    # Retourner la probabilité, la prédiction, les features utilisées, et le seuil avec son nom\n",
    "    return proba, prediction, selected_client[feats], seuil_valeur, seuil_nom_affiche"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f017e6e6-8682-4517-8515-c8dbee6a2df8",
   "metadata": {},
   "source": [
    "def predict(client_id, seuil_nom=\"faible\"):\n",
    "    \"\"\"\n",
    "    Effectue la prédiction du score de crédit pour un client donné.\n",
    "    Retourne la probabilité, la classe prédite (0 ou 1) et les features utilisées.\n",
    "    Utilise un seuil de décision spécifié.\n",
    "    \"\"\"\n",
    "    selected_client = clients_data_scaled.loc[clients_data_scaled['SK_ID_CURR'] == client_id]\n",
    "    \n",
    "    # Assurez-vous que 'selected_client' contient bien un client\n",
    "    if selected_client.empty:\n",
    "        raise ValueError(f\"Aucun client trouvé avec SK_ID_CURR = {client_id}\")\n",
    "    \n",
    "    # Récupérer le seuil correspondant\n",
    "    best_threshold = thresholds[seuil_nom]\n",
    "    \n",
    "    proba = model.predict_proba(selected_client[feats])[0][1]  # Probabilité de la classe positive\n",
    "    # Utilisation du seuil pour la prédiction\n",
    "    prediction = 1 if proba >= best_threshold else 0\n",
    "    \n",
    "    return proba, prediction, selected_client[feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f81023-1b9a-4dbf-a94c-51c3f8785ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_credit(client_id, seuil_nom=\"faible\"):\n",
    "    \"\"\"\n",
    "    Effectue la prédiction du score de crédit et retourne les résultats avec le seuil utilisé.\n",
    "    \"\"\"\n",
    "    # Utilisation de la fonction de prédiction\n",
    "    proba, prediction, features, seuil_utilise, seuil_nom_affiche = predict(client_id, seuil_nom)\n",
    "    \n",
    "    # Convertir numpy types en types Python natifs\n",
    "    proba = float(proba)  # Conversion en float pour la sérialisation JSON\n",
    "    prediction = int(prediction)  # Conversion en int pour la sérialisation JSON\n",
    "    \n",
    "    # Déterminer si le client fait défaut ou non\n",
    "    defaut = \"fait défaut\" if prediction == 1 else \"ne fait pas défaut\"\n",
    "    \n",
    "    # Retourner les résultats avec le seuil utilisé et le nom du seuil\n",
    "    return {\n",
    "        \"result\": prediction,\n",
    "        \"proba\": proba,\n",
    "        \"features\": features.to_dict(),  # Assurez-vous que features soit sous un format sérialisable\n",
    "        \"seuil\": seuil_utilise,\n",
    "        \"seuil_nom\": seuil_nom_affiche,\n",
    "        \"defaut\": defaut  # Retourner si le client fait défaut ou non\n",
    "    }"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c019ab8b-67ce-4e51-ae7c-ad2a0cbeab02",
   "metadata": {},
   "source": [
    "def predict_credit(client_id, seuil_nom=\"faible\"):\n",
    "    # Appelez votre fonction de prédiction ici\n",
    "    proba, prediction, features = predict(client_id, seuil_nom)\n",
    "    \n",
    "    # Récupérer le seuil utilisé\n",
    "    seuil_utilise = thresholds[seuil_nom]\n",
    "    \n",
    "    # Convertir numpy types en types Python natifs\n",
    "    proba = float(proba)  # Conversion en float pour la sérialisation JSON\n",
    "    prediction = int(prediction)  # Conversion en int pour la sérialisation JSON\n",
    "    \n",
    "    return {\n",
    "        \"result\": prediction,\n",
    "        \"proba\": proba,\n",
    "        \"features\": features.to_dict(),  # Assurez-vous que features soit sous un format sérialisable\n",
    "        \"seuil\": seuil_utilise  # Ajouter le seuil utilisé au résultat\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4ac5c0-79e5-4ad8-a971-5ef280283f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'utilisation\n",
    "client_id = 372060  # Remplacez par l'ID du client que vous voulez prédire\n",
    "prediction_result = predict_credit(client_id, seuil_nom=\"faible\")\n",
    "\n",
    "# Afficher uniquement le résultat, la probabilité, le seuil et le nom du seuil\n",
    "result = prediction_result[\"result\"]\n",
    "proba = prediction_result[\"proba\"]\n",
    "seuil = prediction_result[\"seuil\"]\n",
    "seuil_nom = prediction_result[\"seuil_nom\"]\n",
    "defaut = prediction_result[\"defaut\"]\n",
    "\n",
    "print(f\"Résultat de la prédiction : {result} ({defaut})\")\n",
    "print(f\"Probabilité de défaut : {proba:.3f}\")  # Affichage formaté à trois décimales\n",
    "print(f\"Seuil utilisé pour la prédiction : {seuil:.2f} ({seuil_nom})\")  # Affichage du seuil et de son nom"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b250db0-bdba-4f54-9542-0f701a59dd22",
   "metadata": {},
   "source": [
    "# Exemple d'utilisation\n",
    "client_id = 372060  # Remplacez par l'ID du client que vous voulez prédire\n",
    "prediction_result = predict_credit(client_id, seuil_nom=\"sans\")\n",
    "\n",
    "# Assurez-vous que 'selected_client' contient bien un client\n",
    "if selected_client.empty:\n",
    "    raise ValueError(f\"Aucun client trouvé avec SK_ID_CURR = {client_id}\")\n",
    "\n",
    "# Afficher uniquement le résultat, la probabilité et le seuil\n",
    "result = prediction_result[\"result\"]\n",
    "proba = prediction_result[\"proba\"]\n",
    "seuil = prediction_result[\"seuil\"]\n",
    "\n",
    "print(f\"Résultat de la prédiction : {result}\")\n",
    "print(f\"Probabilité de défaut : {proba:.3f}\")  # Affichage formaté à trois décimales\n",
    "print(f\"Seuil utilisé pour la prédiction : {seuil:.2f}\")  # Affichage du seuil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3d229f-baf7-4966-aea3-19f359188d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_client = clients_data_scaled.loc[clients_data_scaled['SK_ID_CURR'] == client_id]\n",
    "selected_client[feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbd8dec-e5f7-4c5c-aa15-7873da3579ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image_to_base64(fig):\n",
    "    \"\"\"Encode une figure Matplotlib en base64.\"\"\"\n",
    "    buffer = BytesIO()\n",
    "    fig.savefig(buffer, format='png', bbox_inches='tight')\n",
    "    buffer.seek(0)\n",
    "    image_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "    buffer.close()\n",
    "    return f\"data:image/png;base64,{image_base64}\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "592d2802-c25f-417e-8b4e-27851958c6bf",
   "metadata": {},
   "source": [
    "def shap_waterfall_chart(selected_client, model, feat_number=10):\n",
    "    \"\"\"\n",
    "    Génère un graphique waterfall SHAP pour un client spécifique.\n",
    "    \"\"\"\n",
    "    # Vérifier que les données du client ne contiennent pas de colonne TARGET ou SK_ID_CURR\n",
    "    selected_client = selected_client.drop(columns=['TARGET', 'SK_ID_CURR'], errors='ignore')\n",
    "    \n",
    "    # Utiliser SHAP TreeExplainer pour le modèle\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    \n",
    "    # Calculer les valeurs SHAP pour les données du client\n",
    "    shap_values = explainer.shap_values(selected_client)\n",
    "    \n",
    "    # Pour les modèles de classification binaire, shap_values est une liste\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[1]  # Classe positive\n",
    "    \n",
    "    # Générer le graphique waterfall SHAP\n",
    "    plt.figure()\n",
    "    shap.waterfall_plot(shap.Explanation(values=shap_values[0],\n",
    "                                          base_values=explainer.expected_value,\n",
    "                                          data=selected_client.iloc[0], \n",
    "                                          feature_names=selected_client.columns),\n",
    "                        max_display=feat_number)\n",
    "    \n",
    "    # Convertir le graphique en image encodée en base64\n",
    "    return encode_image_to_base64(plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacdfa90-d80e-46f1-9fd9-21a9ade54924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_waterfall_chart(selected_client, model, feat_number=10):\n",
    "    \"\"\"\n",
    "    Génère un graphique waterfall SHAP pour un client spécifique.\n",
    "    \"\"\"\n",
    "    # Vérifier que les données du client ne contiennent pas de colonne TARGET ou SK_ID_CURR\n",
    "    selected_client = selected_client.drop(columns=['TARGET', 'SK_ID_CURR'], errors='ignore')\n",
    "    \n",
    "    # Utiliser SHAP TreeExplainer pour le modèle\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    \n",
    "    # Calculer les valeurs SHAP pour les données du client\n",
    "    shap_values = explainer.shap_values(selected_client)\n",
    "    \n",
    "    # Pour les modèles de classification binaire, shap_values est une liste\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[1]  # Classe positive\n",
    "    \n",
    "    # Générer le graphique waterfall SHAP\n",
    "    plt.figure()\n",
    "    shap.waterfall_plot(shap.Explanation(values=shap_values[0],\n",
    "                                          base_values=explainer.expected_value,\n",
    "                                          data=selected_client.iloc[0], \n",
    "                                          feature_names=selected_client.columns),\n",
    "                        max_display=feat_number)\n",
    "    \n",
    "    # Convertir le graphique en image encodée en base64\n",
    "    return encode_image_to_base64(plt)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "52932ab6-6e8c-4228-b852-95f9b5800577",
   "metadata": {},
   "source": [
    "def get_shap_waterfall_chart(client_id: float, feature_count: int = 10):\n",
    "    # Utilisez la fonction predict pour obtenir les caractéristiques et la prédiction\n",
    "    proba, prediction, selected_client = predict(client_id)\n",
    "    \n",
    "    # Vérifiez que 'selected_client' est bien formé pour SHAP\n",
    "    print(f\"Shape de selected_client : {selected_client.shape}\")\n",
    "    #print(f\"Colonnes de selected_client : {selected_client.columns.tolist()}\")\n",
    "\n",
    "    # Créer le graphique waterfall SHAP\n",
    "    shap_chart = shap_waterfall_chart(selected_client, model, feat_number=feature_count)\n",
    "\n",
    "    return {\"shap_chart\": shap_chart, \"probability\": proba, \"prediction\": prediction}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14e4800-197e-46e1-9e9e-74b794518c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shap_waterfall_chart(client_id: float, feature_count: int = 10, seuil_nom=\"modere\"):\n",
    "    \"\"\"\n",
    "    Génère un graphique SHAP waterfall et renvoie la probabilité, la prédiction et le seuil utilisé.\n",
    "    \"\"\"\n",
    "    # Utiliser la fonction predict pour obtenir les caractéristiques, la probabilité, et le seuil\n",
    "    proba, prediction, selected_client, seuil_utilise, seuil_nom_affiche = predict(client_id, seuil_nom=seuil_nom)\n",
    "    \n",
    "    # Vérifiez que 'selected_client' est bien formé pour SHAP\n",
    "    print(f\"Shape de selected_client : {selected_client.shape}\")\n",
    "    print(f\"Seuil utilisé pour la prédiction : {seuil_utilise} ({seuil_nom_affiche})\")\n",
    "    print(f\"Résultat de la prédiction : {result} ({defaut})\")\n",
    "    print(f\"Probabilité de défaut : {proba:.3f} ({proba * 100:.2f}%)\")\n",
    "    \n",
    "    # Créer le graphique waterfall SHAP\n",
    "    shap_chart = shap_waterfall_chart(selected_client, model, feat_number=feature_count)\n",
    "\n",
    "    return {\n",
    "        \"shap_chart\": shap_chart,\n",
    "        \"probability\": proba,\n",
    "        \"prediction\": prediction,\n",
    "        \"seuil_utilise\": seuil_utilise,\n",
    "        \"seuil_nom\": seuil_nom_affiche\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65101796-73df-48de-894f-5eeb7b5721a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_result = get_shap_waterfall_chart(client_id, seuil_nom=\"faible\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ad788eb5-ff25-4b57-a51e-e4b513357c44",
   "metadata": {},
   "source": [
    "# Affichage du graphique SHAP\n",
    "from IPython.display import HTML\n",
    "HTML(f'<img src=\"{prediction_result[\"shap_chart\"]}\"/>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d60528-730f-4e30-ab8e-1fb6e2e84d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier que les données du client ne contiennent pas de colonne TARGET ou SK_ID_CURR\n",
    "selected_client = selected_client.drop(columns=['TARGET', 'SK_ID_CURR'], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0de130-cb00-4b15-9bab-a8b65b302952",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3253b292-4530-4975-863b-e2637ed9d092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_shap_waterfall_chart(client_id: float, model, feat_number=10, seuil_nom=\"modere\"):\n",
    "    \n",
    "    # Exemple de création d'un graphique SHAP\n",
    "    explainer = shap.Explainer(model)\n",
    "    shap_values = explainer(selected_client)\n",
    "    \n",
    "    plt.figure()\n",
    "    shap.waterfall_plot(shap_values[0])  # Visualisation du premier exemple\n",
    "    \n",
    "    \n",
    "    # Sauvegarder le graphique dans un objet BytesIO\n",
    "    buf = BytesIO()\n",
    "    plt.savefig(buf, format='png', bbox_inches='tight')\n",
    "    buf.seek(0)\n",
    "    plt.show()  # Affiche le graphique pour le débogage\n",
    "    # Encoder l'image en base64\n",
    "    shap_chart_base64 = base64.b64encode(buf.getvalue()).decode('utf-8')\n",
    "    \n",
    "    plt.close()  # Fermer le graphique pour éviter les fuites de mémoire\n",
    "    \n",
    "    # Afficher le code en base64 à la fin de la fonction\n",
    "    #print(f\"Graphique SHAP en base64 : {shap_chart_base64}\")\n",
    "    \n",
    "    return {\n",
    "        \"shap_chart\": f\"data:image/png;base64,{shap_chart_base64}\"\n",
    "    }\n",
    "\n",
    "# Utilisation de la fonction\n",
    "prediction_result = get_shap_waterfall_chart(client_id, model, seuil_nom=\"faible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2182009f-4a74-4602-b2f6-540750eb656b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vous pouvez accéder à l'image encodée en base64 ici\n",
    "print(prediction_result[\"shap_chart\"])  # Cela affichera le code encodé en base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6d9fad-9363-4140-9291-a9285c5b5fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd106c5-154f-4ea7-abe3-c6d006059cf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8ae26b-9856-48e5-98e2-2e5de2876aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3040263-0eb0-40b7-97db-1022a2d49ab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce4a2c9c-da7d-48fc-9402-9fe05269048f",
   "metadata": {},
   "source": [
    "### 7.2.1 - Sans thresholds"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fff92679-2052-484e-a860-df91e5fb1f84",
   "metadata": {},
   "source": [
    "def predict(client_id):\n",
    "    \"\"\"\n",
    "    Effectue la prédiction du score de crédit pour un client donné.\n",
    "    Retourne la probabilité, la classe prédite (0 ou 1) et les features utilisées.\n",
    "    \"\"\"\n",
    "    selected_client = clients_data_scaled.loc[clients_data_scaled['SK_ID_CURR'] == client_id]\n",
    "    \n",
    "    # Assurez-vous que 'selected_client' contient bien un client\n",
    "    if selected_client.empty:\n",
    "        raise ValueError(f\"Aucun client trouvé avec SK_ID_CURR = {client_id}\")\n",
    "    \n",
    "    proba = model.predict_proba(selected_client[feats])[0][1]  # Probabilité de la classe positive\n",
    "    prediction = model.predict(selected_client[feats])[0]\n",
    "    \n",
    "    return proba, prediction, selected_client[feats]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6b8188bc-a1f9-44a9-bd99-11564c53038a",
   "metadata": {},
   "source": [
    "def predict_credit(client_id):\n",
    "    # Appelez votre fonction de prédiction ici\n",
    "    proba, prediction, features = predict(client_id)\n",
    "    \n",
    "    # Convertir numpy types en types Python natifs\n",
    "    proba = float(proba)  # Conversion en float pour la sérialisation JSON\n",
    "    prediction = int(prediction)  # Conversion en int pour la sérialisation JSON\n",
    "    \n",
    "    return {\n",
    "        \"result\": prediction,\n",
    "        \"proba\": proba,\n",
    "        \"features\": features.to_dict()  # Assurez-vous que features soit sous un format sérialisable\n",
    "    }"
   ]
  },
  {
   "cell_type": "raw",
   "id": "84215e6c-d68a-4858-9115-df4b5b6d6644",
   "metadata": {},
   "source": [
    "# Exemple d'utilisation\n",
    "client_id = 370323  # Remplacez par l'ID du client que vous voulez prédire\n",
    "prediction_result = predict_credit(client_id)\n",
    "\n",
    "# Afficher uniquement le résultat et la probabilité\n",
    "result = prediction_result[\"result\"]\n",
    "proba = prediction_result[\"proba\"]\n",
    "\n",
    "print(f\"Résultat de la prédiction : {result}\")\n",
    "print(f\"Probabilité de défaut : {proba:.3f}\")  # Affichage formaté à deux décimales"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b28a0431-9801-4252-bc2e-48dea2288c20",
   "metadata": {},
   "source": [
    "selected_client = clients_data_scaled.loc[clients_data_scaled['SK_ID_CURR'] == client_id]\n",
    "    \n",
    "# Assurez-vous que 'selected_client' contient bien un client\n",
    "if selected_client.empty:\n",
    "    raise ValueError(f\"Aucun client trouvé avec SK_ID_CURR = {client_id}\")\n",
    "    \n",
    "proba = model.predict_proba(selected_client[feats])[0][1]  # Probabilité de la classe positive\n",
    "prediction = model.predict(selected_client[feats])[0]\n",
    "    \n",
    "proba, prediction, selected_client[feats]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "da5dfef9-a9d0-4a94-bea9-e8af7f5d7078",
   "metadata": {},
   "source": [
    "selected_client = selected_client.drop(columns=['SK_ID_CURR', 'TARGET'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e6973a27-9df8-4c07-bafa-ebf22cc33231",
   "metadata": {},
   "source": [
    "selected_client"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2ad49c40-23f3-4dde-b1f5-b0de7760b86b",
   "metadata": {},
   "source": [
    "def encode_image_to_base64(fig):\n",
    "    \"\"\"Encode une figure Matplotlib en base64.\"\"\"\n",
    "    buffer = BytesIO()\n",
    "    fig.savefig(buffer, format='png', bbox_inches='tight')\n",
    "    buffer.seek(0)\n",
    "    image_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "    buffer.close()\n",
    "    return f\"data:image/png;base64,{image_base64}\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "ddd6435e-98e6-40c6-9c85-3721e8d34cd0",
   "metadata": {},
   "source": [
    "def shap_waterfall_chart(selected_client, model, feat_number=10):\n",
    "    \"\"\"\n",
    "    Génère un graphique waterfall SHAP pour un client spécifique.\n",
    "    \"\"\"\n",
    "    # Vérifier que les données du client ne contiennent pas de colonne TARGET ou SK_ID_CURR\n",
    "    selected_client = selected_client.drop(columns=['TARGET', 'SK_ID_CURR'], errors='ignore')\n",
    "    \n",
    "    # Utiliser SHAP TreeExplainer pour le modèle\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    \n",
    "    # Calculer les valeurs SHAP pour les données du client\n",
    "    shap_values = explainer.shap_values(selected_client)\n",
    "    \n",
    "    # Pour les modèles de classification binaire, shap_values est une liste\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[1]  # Classe positive\n",
    "    \n",
    "    # Générer le graphique waterfall SHAP\n",
    "    plt.figure()\n",
    "    shap.waterfall_plot(shap.Explanation(values=shap_values[0],\n",
    "                                          base_values=explainer.expected_value,\n",
    "                                          data=selected_client.iloc[0], \n",
    "                                          feature_names=selected_client.columns),\n",
    "                        max_display=feat_number)\n",
    "    \n",
    "    # Convertir le graphique en image encodée en base64\n",
    "    return encode_image_to_base64(plt)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d5fe31c7-8fc1-41c6-a8ad-53fcfb0d3103",
   "metadata": {},
   "source": [
    "def get_shap_waterfall_chart(client_id: float, feature_count: int = 10):\n",
    "    # Utilisez la fonction predict pour obtenir les caractéristiques et la prédiction\n",
    "    proba, prediction, selected_client = predict(client_id)\n",
    "    \n",
    "    # Vérifiez que 'selected_client' est bien formé pour SHAP\n",
    "    print(f\"Shape de selected_client : {selected_client.shape}\")\n",
    "    print(f\"Colonnes de selected_client : {selected_client.columns.tolist()}\")\n",
    "\n",
    "    # Créer le graphique waterfall SHAP\n",
    "    shap_chart = shap_waterfall_chart(selected_client, model, feat_number=feature_count)\n",
    "\n",
    "    return {\"shap_chart\": shap_chart, \"probability\": proba, \"prediction\": prediction}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "17e2bec9-1a1c-4f6a-9dd2-a3b9e0a64725",
   "metadata": {},
   "source": [
    "# Vérifier les valeurs manquantes dans le DataFrame selected_client\n",
    "missing_values = selected_client.isnull().sum()\n",
    "\n",
    "# Afficher les colonnes avec des valeurs manquantes\n",
    "missing_values[missing_values > 0]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6c5e00c2-29b1-4e78-bb05-035549c96596",
   "metadata": {},
   "source": [
    "prediction_result = get_shap_waterfall_chart(client_id)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "24c48695-82eb-4544-9812-e6f4737c6485",
   "metadata": {},
   "source": [
    "# Afficher uniquement le résultat et la probabilité\n",
    "result = prediction_result[\"prediction\"]\n",
    "proba = prediction_result[\"probability\"]\n",
    "\n",
    "print(f\"Résultat de la prédiction : {result}\")\n",
    "print(f\"Probabilité de défaut : {proba:.3f}\")  # Affichage formaté à trois décimales"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c926f87b-e1a3-4aa1-930b-d7dd6eacf189",
   "metadata": {},
   "source": [
    "# Affichage du graphique SHAP\n",
    "from IPython.display import HTML\n",
    "HTML(f'<img src=\"{prediction_result[\"shap_chart\"]}\"/>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245aa030-aff6-484d-9517-9f66f4bc62db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1278e2f1-9d48-4546-bf31-010dd36460fb",
   "metadata": {},
   "source": [
    "## 7.3 - Features importance globale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7202f29e-cafc-4ede-b1fc-51d64c313be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clients_data_scaled_global = clients_data_scaled.drop(columns=['SK_ID_CURR', 'TARGET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a01e5a-bb04-4def-bfa7-38c20eaec7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un explainer SHAP pour le modèle\n",
    "explainer = shap.TreeExplainer(model)\n",
    "\n",
    "# Calculer les valeurs SHAP pour l'ensemble du jeu de données\n",
    "shap_values = explainer.shap_values(clients_data_scaled_global)\n",
    "\n",
    "# Calculer l'importance globale des caractéristiques\n",
    "importance_df = pd.DataFrame(shap_values, columns=clients_data_scaled_global.columns)\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': importance_df.columns,\n",
    "    'Importance': importance_df.abs().mean()\n",
    "}).sort_values(by='Importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf8f32e-a5d8-489a-a18c-17df8acdce8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Afficher l'importance des caractéristiques\n",
    "importance_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e6d224-c423-445b-8b42-51c381551ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser l'importance des caractéristiques avec un bar plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=importance_df.head(20))  # Afficher les 20 premières\n",
    "plt.title('Importance Globale des Caractéristiques (SHAP)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802a05e0-858b-4322-bc83-207748a247c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6dd2a57d-0c6f-460c-b460-aac72c647434",
   "metadata": {},
   "source": [
    "# 8 - Datadrift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64287ac-b034-4e80-b1b9-3bb609e0ce43",
   "metadata": {},
   "source": [
    "Le Data Drift fait référence aux changements dans la distribution des données au fil du temps. Il peut se produire lorsque les données utilisées pour entraîner un modèle ne reflètent plus la réalité des données qui entrent en production. Cela peut avoir un impact significatif sur les performances des modèles prédictifs, car ils sont souvent sensibles aux variations dans les données d'entrée.\n",
    "\n",
    "On va donc tester une étape supplémentaire dans le cycle MLOps pour anticiper la surveillance de la performance du modèle en production. En particulier, nous allons utiliser la librairie Evidently pour analyser le Data Drift, c’est-à-dire les changements dans la distribution des données entre le moment où le modèle est entraîné et lorsque de nouvelles données arrivent en production.\n",
    "\n",
    "**1. Hypothèse de test**\n",
    "- **\"application_train\"** représente les données d'entraînement.\n",
    "- **\"application_test\"** représente les nouvelles données des clients une fois le modèle en production.\n",
    "\n",
    "**2. Tâche**\n",
    "- Nous allons utiliser la librairie **Evidently** pour comparer les données de ces deux ensembles (train et test) et générer un rapport HTML pour détecter des différences ou du *Data Drift* sur les principales caractéristiques du modèle.\n",
    "\n",
    "**3. Étapes à suivre**\n",
    "- Charger les jeux de données `application_train` et `application_test`.\n",
    "- Utiliser **Evidently** pour créer un rapport de comparaison entre les deux datasets.\n",
    "- Le rapport mettra en évidence les colonnes où un *drift* est détecté."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1f6e2e3-8963-4c54-8a0e-734edbfd8958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un rapport de Data Drift\n",
    "data_drift_report = Report(metrics=[DataDriftTable()])\n",
    "\n",
    "# Exécuter le rapport avec les données de référence et les données actuelles\n",
    "data_drift_report.run(reference_data=df_train, current_data=df_test)\n",
    "\n",
    "# Exporter le rapport en HTML pour visualisation\n",
    "data_drift_report.save_html(\"data_drift_report.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f10fd7d-0874-44d1-a5f3-72bd31f9f139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"600\"\n",
       "            src=\"data_drift_report.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1ea8a8717d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Afficher le rapport HTML dans le notebook\n",
    "IFrame('data_drift_report.html', width=900, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acf88ce-a854-4998-8d2a-d5b2d7c11581",
   "metadata": {},
   "source": [
    "**Drift Détecté** : 9.61 % des colonnes (32 sur 333) présentent un drift. Cela signifie que les distributions des données d'entraînement et de test (ou de production) ont évolué pour ces colonnes, ce qui peut affecter la performance de notre modèle. La majorité des colonnes détectées pour le drift semblent liées à des caractéristiques extraites de la variable `DAYS_BIRTH` et à des variables externes (`EXT_SOURCE_1`, `EXT_SOURCE_2`, `EXT_SOURCE_3`).\n",
    "\n",
    "1. **Colonnes avec le Plus Grand Drift** :\n",
    "   - `DAYS_BIRTH` a la distance de Wasserstein normée la plus élevée (7.356832), indiquant un drift significatif.\n",
    "   - D'autres colonnes comme `EXT_SOURCE_1`, `EXT_SOURCE_2`, et `EXT_SOURCE_3` présentent également des valeurs élevées, suggérant des changements notables dans leurs distributions.\n",
    "\n",
    "2. **Distance de Wasserstein** :\n",
    "   - La distance de Wasserstein est une mesure de la différence entre deux distributions de probabilité. Plus la valeur est élevée, plus la différence entre les distributions d'entraînement et de test est significative.\n",
    "   - Par exemple, des distances entre 4 et 7, comme celles observées pour `DAYS_BIRTH` et les variables externes, suggèrent un drift potentiellement préoccupant.\n",
    "\n",
    "**Implications du Data Drift**\n",
    "\n",
    "- **Impact sur le Modèle** : Un drift dans des variables clés pourrait indiquer que le modèle n’est plus représentatif des données actuelles, ce qui pourrait entraîner une baisse de performance. Les variables comme `DAYS_BIRTH` sont souvent essentielles dans des modèles de scoring de crédit.\n",
    "\n",
    "- **Actions potentieles** :\n",
    "  1. **Réévaluation du Modèle** : Considérer le réentraînement du modèle avec des données récentes si le drift est significatif. Cela pourrait inclure l'ajout de nouvelles données ou l’actualisation des features.\n",
    "  \n",
    "  2. **Surveillance Continue** : Mettre en place des alertes pour suivre le drift sur ces colonnes afin d'anticiper les problèmes avant qu'ils n'affectent significativement la performance du modèle.\n",
    "\n",
    "  3. **Analyse Plus Profonde** : Effectuer une analyse plus approfondie des colonnes spécifiques qui montrent un drift. Cela peut inclure des visualisations pour voir comment les distributions ont changé.\n",
    "\n",
    "Le Data Drift est une indication que les conditions sous-jacentes qui ont conduit à l'entraînement initial du modèle peuvent avoir changé. En suivant les recommandations et en surveillant de près les colonnes concernées, nous pouvons mieux maintenir et ajuster notre modèle pour qu'il reste performant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d92e958-1de4-46eb-b266-8f58408f157b",
   "metadata": {},
   "source": [
    "# 9 - Déploiement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fdb60f-3d8b-4e62-9a0a-afec5e2e8ddb",
   "metadata": {},
   "source": [
    "Démarrer son API en local avec uvicorn api:app --reload\n",
    "\n",
    "On peut aller sur le Swagger FastAPI ici : http://127.0.0.1:8000/docs#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d428e3-7802-4851-a95e-9ad6049ad628",
   "metadata": {},
   "source": [
    "Pour deploier notre modèle nous avons construit un fichier utils.py qui contient les fonctions utilisées dans api.py. Un autre fichier interface.py permettra de construire le dashboard streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0d4aa6-f0d4-47d8-a67e-51e851a705a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aab819-caa1-4b68-a8b5-61ece31a40ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773ab627-5ad7-408b-b714-ffbea22a0486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4fde5d-b397-4acf-b3ab-1e8e6bf434f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a9e548-9dd2-4b25-899a-29bd29286296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a63c68-6186-4783-9320-ee01077a93fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de6f635-7860-4333-842c-99a336ee1bef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddb2c412-33b3-40df-b781-25e1de53a3e6",
   "metadata": {},
   "source": [
    "# 10 - Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5a73aa-9bed-4d81-94e4-fc54c2474133",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f213f1-1eb9-4fa4-8db1-430e6adc2a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_time = time.time() - start  # Calcule le temps écoulé\n",
    "print(f\"Temps total d'exécution du notebook : {elapsed_time:.2f} secondes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f94662-b778-43b7-b5e4-687496e52534",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
